{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Questions_Internal_R6_AIML_Labs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUZjPnVXGz0Z",
        "colab_type": "text"
      },
      "source": [
        "# The Iris Dataset\n",
        "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n",
        "\n",
        "The dataset contains a set of 150 records under five attributes - petal length, petal width, sepal length, sepal width and species."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOk8Eu4_t70R",
        "colab_type": "text"
      },
      "source": [
        "Firstly, let's select TensorFlow version 2.x in colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6RZUm0p4wYJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ea4f05e5-b2b3-4b82-e7d0-d56b1153497d"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow\n",
        "tensorflow.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWi96z-8SyX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize the random number generator\n",
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "# Ignore the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-vVQBBqg7DI",
        "colab_type": "text"
      },
      "source": [
        "## Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE0EDKvQhEIe",
        "colab_type": "text"
      },
      "source": [
        "### Import dataset\n",
        "- Import iris dataset\n",
        "- Import the dataset using sklearn library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOOWpD26Haq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wzbQo0DXLNu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "81df165f-2c3e-439f-eb8d-042a81f0dfe0"
      },
      "source": [
        "import pandas as pd\n",
        "IR = pd.DataFrame(iris.data)\n",
        "IR.columns = iris.feature_names\n",
        "IR['CLASS'] = iris.target\n",
        "IR.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "      <th>CLASS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  ...  petal width (cm)  CLASS\n",
              "0                5.1               3.5  ...               0.2      0\n",
              "1                4.9               3.0  ...               0.2      0\n",
              "2                4.7               3.2  ...               0.2      0\n",
              "3                4.6               3.1  ...               0.2      0\n",
              "4                5.0               3.6  ...               0.2      0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta8YqInTh5v5",
        "colab_type": "text"
      },
      "source": [
        "## Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HERt3drbhX0i",
        "colab_type": "text"
      },
      "source": [
        "### Get features and label from the dataset in separate variable\n",
        "- you can get the features using .data method\n",
        "- you can get the features using .target method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cV-_qHAHyvE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "96813b1c-ee2e-4495-e1fb-f9d232c47cd2"
      },
      "source": [
        "features = iris.data[: , [0,1,2,3]]\n",
        "features"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2],\n",
              "       [5.4, 3.9, 1.7, 0.4],\n",
              "       [4.6, 3.4, 1.4, 0.3],\n",
              "       [5. , 3.4, 1.5, 0.2],\n",
              "       [4.4, 2.9, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.1],\n",
              "       [5.4, 3.7, 1.5, 0.2],\n",
              "       [4.8, 3.4, 1.6, 0.2],\n",
              "       [4.8, 3. , 1.4, 0.1],\n",
              "       [4.3, 3. , 1.1, 0.1],\n",
              "       [5.8, 4. , 1.2, 0.2],\n",
              "       [5.7, 4.4, 1.5, 0.4],\n",
              "       [5.4, 3.9, 1.3, 0.4],\n",
              "       [5.1, 3.5, 1.4, 0.3],\n",
              "       [5.7, 3.8, 1.7, 0.3],\n",
              "       [5.1, 3.8, 1.5, 0.3],\n",
              "       [5.4, 3.4, 1.7, 0.2],\n",
              "       [5.1, 3.7, 1.5, 0.4],\n",
              "       [4.6, 3.6, 1. , 0.2],\n",
              "       [5.1, 3.3, 1.7, 0.5],\n",
              "       [4.8, 3.4, 1.9, 0.2],\n",
              "       [5. , 3. , 1.6, 0.2],\n",
              "       [5. , 3.4, 1.6, 0.4],\n",
              "       [5.2, 3.5, 1.5, 0.2],\n",
              "       [5.2, 3.4, 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.6, 0.2],\n",
              "       [4.8, 3.1, 1.6, 0.2],\n",
              "       [5.4, 3.4, 1.5, 0.4],\n",
              "       [5.2, 4.1, 1.5, 0.1],\n",
              "       [5.5, 4.2, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.2, 1.2, 0.2],\n",
              "       [5.5, 3.5, 1.3, 0.2],\n",
              "       [4.9, 3.6, 1.4, 0.1],\n",
              "       [4.4, 3. , 1.3, 0.2],\n",
              "       [5.1, 3.4, 1.5, 0.2],\n",
              "       [5. , 3.5, 1.3, 0.3],\n",
              "       [4.5, 2.3, 1.3, 0.3],\n",
              "       [4.4, 3.2, 1.3, 0.2],\n",
              "       [5. , 3.5, 1.6, 0.6],\n",
              "       [5.1, 3.8, 1.9, 0.4],\n",
              "       [4.8, 3. , 1.4, 0.3],\n",
              "       [5.1, 3.8, 1.6, 0.2],\n",
              "       [4.6, 3.2, 1.4, 0.2],\n",
              "       [5.3, 3.7, 1.5, 0.2],\n",
              "       [5. , 3.3, 1.4, 0.2],\n",
              "       [7. , 3.2, 4.7, 1.4],\n",
              "       [6.4, 3.2, 4.5, 1.5],\n",
              "       [6.9, 3.1, 4.9, 1.5],\n",
              "       [5.5, 2.3, 4. , 1.3],\n",
              "       [6.5, 2.8, 4.6, 1.5],\n",
              "       [5.7, 2.8, 4.5, 1.3],\n",
              "       [6.3, 3.3, 4.7, 1.6],\n",
              "       [4.9, 2.4, 3.3, 1. ],\n",
              "       [6.6, 2.9, 4.6, 1.3],\n",
              "       [5.2, 2.7, 3.9, 1.4],\n",
              "       [5. , 2. , 3.5, 1. ],\n",
              "       [5.9, 3. , 4.2, 1.5],\n",
              "       [6. , 2.2, 4. , 1. ],\n",
              "       [6.1, 2.9, 4.7, 1.4],\n",
              "       [5.6, 2.9, 3.6, 1.3],\n",
              "       [6.7, 3.1, 4.4, 1.4],\n",
              "       [5.6, 3. , 4.5, 1.5],\n",
              "       [5.8, 2.7, 4.1, 1. ],\n",
              "       [6.2, 2.2, 4.5, 1.5],\n",
              "       [5.6, 2.5, 3.9, 1.1],\n",
              "       [5.9, 3.2, 4.8, 1.8],\n",
              "       [6.1, 2.8, 4. , 1.3],\n",
              "       [6.3, 2.5, 4.9, 1.5],\n",
              "       [6.1, 2.8, 4.7, 1.2],\n",
              "       [6.4, 2.9, 4.3, 1.3],\n",
              "       [6.6, 3. , 4.4, 1.4],\n",
              "       [6.8, 2.8, 4.8, 1.4],\n",
              "       [6.7, 3. , 5. , 1.7],\n",
              "       [6. , 2.9, 4.5, 1.5],\n",
              "       [5.7, 2.6, 3.5, 1. ],\n",
              "       [5.5, 2.4, 3.8, 1.1],\n",
              "       [5.5, 2.4, 3.7, 1. ],\n",
              "       [5.8, 2.7, 3.9, 1.2],\n",
              "       [6. , 2.7, 5.1, 1.6],\n",
              "       [5.4, 3. , 4.5, 1.5],\n",
              "       [6. , 3.4, 4.5, 1.6],\n",
              "       [6.7, 3.1, 4.7, 1.5],\n",
              "       [6.3, 2.3, 4.4, 1.3],\n",
              "       [5.6, 3. , 4.1, 1.3],\n",
              "       [5.5, 2.5, 4. , 1.3],\n",
              "       [5.5, 2.6, 4.4, 1.2],\n",
              "       [6.1, 3. , 4.6, 1.4],\n",
              "       [5.8, 2.6, 4. , 1.2],\n",
              "       [5. , 2.3, 3.3, 1. ],\n",
              "       [5.6, 2.7, 4.2, 1.3],\n",
              "       [5.7, 3. , 4.2, 1.2],\n",
              "       [5.7, 2.9, 4.2, 1.3],\n",
              "       [6.2, 2.9, 4.3, 1.3],\n",
              "       [5.1, 2.5, 3. , 1.1],\n",
              "       [5.7, 2.8, 4.1, 1.3],\n",
              "       [6.3, 3.3, 6. , 2.5],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [7.1, 3. , 5.9, 2.1],\n",
              "       [6.3, 2.9, 5.6, 1.8],\n",
              "       [6.5, 3. , 5.8, 2.2],\n",
              "       [7.6, 3. , 6.6, 2.1],\n",
              "       [4.9, 2.5, 4.5, 1.7],\n",
              "       [7.3, 2.9, 6.3, 1.8],\n",
              "       [6.7, 2.5, 5.8, 1.8],\n",
              "       [7.2, 3.6, 6.1, 2.5],\n",
              "       [6.5, 3.2, 5.1, 2. ],\n",
              "       [6.4, 2.7, 5.3, 1.9],\n",
              "       [6.8, 3. , 5.5, 2.1],\n",
              "       [5.7, 2.5, 5. , 2. ],\n",
              "       [5.8, 2.8, 5.1, 2.4],\n",
              "       [6.4, 3.2, 5.3, 2.3],\n",
              "       [6.5, 3. , 5.5, 1.8],\n",
              "       [7.7, 3.8, 6.7, 2.2],\n",
              "       [7.7, 2.6, 6.9, 2.3],\n",
              "       [6. , 2.2, 5. , 1.5],\n",
              "       [6.9, 3.2, 5.7, 2.3],\n",
              "       [5.6, 2.8, 4.9, 2. ],\n",
              "       [7.7, 2.8, 6.7, 2. ],\n",
              "       [6.3, 2.7, 4.9, 1.8],\n",
              "       [6.7, 3.3, 5.7, 2.1],\n",
              "       [7.2, 3.2, 6. , 1.8],\n",
              "       [6.2, 2.8, 4.8, 1.8],\n",
              "       [6.1, 3. , 4.9, 1.8],\n",
              "       [6.4, 2.8, 5.6, 2.1],\n",
              "       [7.2, 3. , 5.8, 1.6],\n",
              "       [7.4, 2.8, 6.1, 1.9],\n",
              "       [7.9, 3.8, 6.4, 2. ],\n",
              "       [6.4, 2.8, 5.6, 2.2],\n",
              "       [6.3, 2.8, 5.1, 1.5],\n",
              "       [6.1, 2.6, 5.6, 1.4],\n",
              "       [7.7, 3. , 6.1, 2.3],\n",
              "       [6.3, 3.4, 5.6, 2.4],\n",
              "       [6.4, 3.1, 5.5, 1.8],\n",
              "       [6. , 3. , 4.8, 1.8],\n",
              "       [6.9, 3.1, 5.4, 2.1],\n",
              "       [6.7, 3.1, 5.6, 2.4],\n",
              "       [6.9, 3.1, 5.1, 2.3],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [6.8, 3.2, 5.9, 2.3],\n",
              "       [6.7, 3.3, 5.7, 2.5],\n",
              "       [6.7, 3. , 5.2, 2.3],\n",
              "       [6.3, 2.5, 5. , 1.9],\n",
              "       [6.5, 3. , 5.2, 2. ],\n",
              "       [6.2, 3.4, 5.4, 2.3],\n",
              "       [5.9, 3. , 5.1, 1.8]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZGk_8ZqaEvo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "a03401ea-0a02-4d04-ecb8-f940fcae79c8"
      },
      "source": [
        "targets = iris.target\n",
        "targets"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg1A2lkUjFak",
        "colab_type": "text"
      },
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3YErwYLCH0N_"
      },
      "source": [
        "### Create train and test data\n",
        "- use train_test_split to get train and test set\n",
        "- set a random_state: 1\n",
        "- test_size: 0.25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYKNJL85h7pQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import `train_test_split` from `sklearn.model_selection`\n",
        "import numpy as np \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Specify the data \n",
        "X=IR.iloc[:,0:4]\n",
        "\n",
        "# Specify the target labels and flatten the array\n",
        "y= np.ravel(IR.CLASS)\n",
        "\n",
        "# Split the data up in train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-Xd9X9sdPbN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "fcff5687-a3f0-4f23-feec-10e2bfdc92a1"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>6.5</td>\n",
              "      <td>2.8</td>\n",
              "      <td>4.6</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>6.7</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.8</td>\n",
              "      <td>1.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>6.8</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.5</td>\n",
              "      <td>2.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>6.0</td>\n",
              "      <td>2.2</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.8</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>6.4</td>\n",
              "      <td>3.1</td>\n",
              "      <td>5.5</td>\n",
              "      <td>1.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>4.9</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.1</td>\n",
              "      <td>5.6</td>\n",
              "      <td>2.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>112 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
              "54                 6.5               2.8                4.6               1.5\n",
              "108                6.7               2.5                5.8               1.8\n",
              "112                6.8               3.0                5.5               2.1\n",
              "17                 5.1               3.5                1.4               0.3\n",
              "119                6.0               2.2                5.0               1.5\n",
              "..                 ...               ...                ...               ...\n",
              "133                6.3               2.8                5.1               1.5\n",
              "137                6.4               3.1                5.5               1.8\n",
              "72                 6.3               2.5                4.9               1.5\n",
              "140                6.7               3.1                5.6               2.4\n",
              "37                 4.9               3.6                1.4               0.1\n",
              "\n",
              "[112 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUsLXXWLbg1p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "8ef6d123-2064-4d59-8303-98df7794ffe4"
      },
      "source": [
        "y"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK8oz5mZbkfZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "c917832c-38dc-4fbb-b056-06c4dd5057bb"
      },
      "source": [
        "IR.CLASS"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0\n",
              "1      0\n",
              "2      0\n",
              "3      0\n",
              "4      0\n",
              "      ..\n",
              "145    2\n",
              "146    2\n",
              "147    2\n",
              "148    2\n",
              "149    2\n",
              "Name: CLASS, Length: 150, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0KVP17Ozaix",
        "colab_type": "text"
      },
      "source": [
        "## Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIjqxbhWv1zv",
        "colab_type": "text"
      },
      "source": [
        "### One-hot encode the labels\n",
        "- convert class vectors (integers) to binary class matrix\n",
        "- convert labels\n",
        "- number of classes: 3\n",
        "- we are doing this to use categorical_crossentropy as loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R9vv-_gpyLY9",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "ytrain = keras.utils.to_categorical(y_train, 3)\n",
        "ytest = keras.utils.to_categorical(y_test, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9Uu7NGsce6Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca881e76-ab9d-4b96-cc43-33d647f80439"
      },
      "source": [
        "ytrain"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ovjLyYzWkO9s"
      },
      "source": [
        "## Question 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbIFzoPNSyYo",
        "colab_type": "text"
      },
      "source": [
        "### Initialize a sequential model\n",
        "- Define a sequential model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FvSbf1UjHtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.layers.advanced_activations import ReLU\n",
        "\n",
        "# define the model architecture\n",
        "\n",
        "# Initialize the constructor\n",
        "model = Sequential()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dGMy999vlacX"
      },
      "source": [
        "## Question 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72ibK5Jxm8iL",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "- Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3\n",
        "- Apply Softmax on Dense Layer outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZKrBNSRm_o9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add an input layer  \n",
        "model.add(Dense(30, activation='relu', input_shape=(4,)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oyy2uSxNd50k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Dense(3,activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKrTUl_oekB_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9638bdcf-e250-4ff4-bef8-405ba6f15066"
      },
      "source": [
        "X_train.size"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "448"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gfcV6xvfBbc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01632548-8414-4c45-ac5f-bbfddf5764c2"
      },
      "source": [
        "y_train.size"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "112"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4uiTH8plmNX",
        "colab_type": "text"
      },
      "source": [
        "## Question 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJL8n8vcSyYz",
        "colab_type": "text"
      },
      "source": [
        "### Compile the model\n",
        "- Use SGD as Optimizer\n",
        "- Use categorical_crossentropy as loss function\n",
        "- Use accuracy as metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc_-fjIEk1ve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sihIGbRll_jT"
      },
      "source": [
        "## Question 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54ZZCfNGlu0i",
        "colab_type": "text"
      },
      "source": [
        "### Summarize the model\n",
        "- Check model layers\n",
        "- Understand number of trainable parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elER3F_4ln8n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "80517efc-5f37-45b2-d8ba-031d8fe2a7b0"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_26 (Dense)             (None, 30)                150       \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 3)                 93        \n",
            "=================================================================\n",
            "Total params: 243\n",
            "Trainable params: 243\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2PiP7j3Vmj4p"
      },
      "source": [
        "## Question 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWdbfFCXmCHt",
        "colab_type": "text"
      },
      "source": [
        "### Fit the model\n",
        "- Give train data as training features and labels\n",
        "- Epochs: 100\n",
        "- Give validation data as testing features and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWiNGE6rq3lR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "es=EarlyStopping(monitor='val_loss',mode='min',verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO1c-5tjmBVZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f54420f8-56fe-45bb-c3fd-53c57e5a4680"
      },
      "source": [
        "epochs = 600\n",
        "batch_size = 224\n",
        "\n",
        "history = model.fit(X_train, ytrain, batch_size=batch_size, epochs=epochs, validation_split=.3, verbose=True,callbacks=[es])\n",
        "loss,accuracy  = model.evaluate(X_test, ytest, verbose=False)"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 78 samples, validate on 34 samples\n",
            "Epoch 1/600\n",
            "78/78 [==============================] - 0s 5ms/sample - loss: 3.7501 - accuracy: 0.3333 - val_loss: 2.7784 - val_accuracy: 0.3235\n",
            "Epoch 2/600\n",
            "78/78 [==============================] - 0s 160us/sample - loss: 3.1341 - accuracy: 0.3333 - val_loss: 2.3278 - val_accuracy: 0.3235\n",
            "Epoch 3/600\n",
            "78/78 [==============================] - 0s 165us/sample - loss: 2.6091 - accuracy: 0.3333 - val_loss: 1.9582 - val_accuracy: 0.3235\n",
            "Epoch 4/600\n",
            "78/78 [==============================] - 0s 145us/sample - loss: 2.1773 - accuracy: 0.3333 - val_loss: 1.6841 - val_accuracy: 0.3235\n",
            "Epoch 5/600\n",
            "78/78 [==============================] - 0s 169us/sample - loss: 1.8549 - accuracy: 0.3333 - val_loss: 1.4706 - val_accuracy: 0.3235\n",
            "Epoch 6/600\n",
            "78/78 [==============================] - 0s 162us/sample - loss: 1.6005 - accuracy: 0.3333 - val_loss: 1.3056 - val_accuracy: 0.3235\n",
            "Epoch 7/600\n",
            "78/78 [==============================] - 0s 143us/sample - loss: 1.3988 - accuracy: 0.3333 - val_loss: 1.1783 - val_accuracy: 0.3235\n",
            "Epoch 8/600\n",
            "78/78 [==============================] - 0s 150us/sample - loss: 1.2413 - accuracy: 0.3333 - val_loss: 1.0806 - val_accuracy: 0.3235\n",
            "Epoch 9/600\n",
            "78/78 [==============================] - 0s 162us/sample - loss: 1.1182 - accuracy: 0.3333 - val_loss: 1.0057 - val_accuracy: 0.3235\n",
            "Epoch 10/600\n",
            "78/78 [==============================] - 0s 174us/sample - loss: 1.0219 - accuracy: 0.3333 - val_loss: 0.9482 - val_accuracy: 0.3235\n",
            "Epoch 11/600\n",
            "78/78 [==============================] - 0s 140us/sample - loss: 0.9469 - accuracy: 0.3333 - val_loss: 0.9048 - val_accuracy: 0.3235\n",
            "Epoch 12/600\n",
            "78/78 [==============================] - 0s 162us/sample - loss: 0.8907 - accuracy: 0.3333 - val_loss: 0.8733 - val_accuracy: 0.3529\n",
            "Epoch 13/600\n",
            "78/78 [==============================] - 0s 145us/sample - loss: 0.8497 - accuracy: 0.3333 - val_loss: 0.8492 - val_accuracy: 0.5588\n",
            "Epoch 14/600\n",
            "78/78 [==============================] - 0s 165us/sample - loss: 0.8169 - accuracy: 0.6282 - val_loss: 0.8301 - val_accuracy: 0.6176\n",
            "Epoch 15/600\n",
            "78/78 [==============================] - 0s 217us/sample - loss: 0.7905 - accuracy: 0.7308 - val_loss: 0.8166 - val_accuracy: 0.6176\n",
            "Epoch 16/600\n",
            "78/78 [==============================] - 0s 212us/sample - loss: 0.7710 - accuracy: 0.7308 - val_loss: 0.8055 - val_accuracy: 0.6176\n",
            "Epoch 17/600\n",
            "78/78 [==============================] - 0s 181us/sample - loss: 0.7544 - accuracy: 0.7308 - val_loss: 0.7963 - val_accuracy: 0.6176\n",
            "Epoch 18/600\n",
            "78/78 [==============================] - 0s 139us/sample - loss: 0.7399 - accuracy: 0.7308 - val_loss: 0.7879 - val_accuracy: 0.6176\n",
            "Epoch 19/600\n",
            "78/78 [==============================] - 0s 170us/sample - loss: 0.7271 - accuracy: 0.7308 - val_loss: 0.7806 - val_accuracy: 0.6176\n",
            "Epoch 20/600\n",
            "78/78 [==============================] - 0s 198us/sample - loss: 0.7158 - accuracy: 0.7308 - val_loss: 0.7742 - val_accuracy: 0.6176\n",
            "Epoch 21/600\n",
            "78/78 [==============================] - 0s 209us/sample - loss: 0.7058 - accuracy: 0.7308 - val_loss: 0.7685 - val_accuracy: 0.6176\n",
            "Epoch 22/600\n",
            "78/78 [==============================] - 0s 200us/sample - loss: 0.6970 - accuracy: 0.7308 - val_loss: 0.7633 - val_accuracy: 0.6176\n",
            "Epoch 23/600\n",
            "78/78 [==============================] - 0s 200us/sample - loss: 0.6891 - accuracy: 0.7308 - val_loss: 0.7582 - val_accuracy: 0.6176\n",
            "Epoch 24/600\n",
            "78/78 [==============================] - 0s 168us/sample - loss: 0.6819 - accuracy: 0.7308 - val_loss: 0.7533 - val_accuracy: 0.6176\n",
            "Epoch 25/600\n",
            "78/78 [==============================] - 0s 187us/sample - loss: 0.6751 - accuracy: 0.7308 - val_loss: 0.7484 - val_accuracy: 0.6176\n",
            "Epoch 26/600\n",
            "78/78 [==============================] - 0s 153us/sample - loss: 0.6688 - accuracy: 0.7308 - val_loss: 0.7436 - val_accuracy: 0.6176\n",
            "Epoch 27/600\n",
            "78/78 [==============================] - 0s 212us/sample - loss: 0.6628 - accuracy: 0.7308 - val_loss: 0.7390 - val_accuracy: 0.6176\n",
            "Epoch 28/600\n",
            "78/78 [==============================] - 0s 163us/sample - loss: 0.6571 - accuracy: 0.7308 - val_loss: 0.7343 - val_accuracy: 0.6176\n",
            "Epoch 29/600\n",
            "78/78 [==============================] - 0s 150us/sample - loss: 0.6515 - accuracy: 0.7308 - val_loss: 0.7299 - val_accuracy: 0.6176\n",
            "Epoch 30/600\n",
            "78/78 [==============================] - 0s 203us/sample - loss: 0.6462 - accuracy: 0.7308 - val_loss: 0.7255 - val_accuracy: 0.6176\n",
            "Epoch 31/600\n",
            "78/78 [==============================] - 0s 244us/sample - loss: 0.6411 - accuracy: 0.7308 - val_loss: 0.7212 - val_accuracy: 0.6176\n",
            "Epoch 32/600\n",
            "78/78 [==============================] - 0s 270us/sample - loss: 0.6362 - accuracy: 0.7308 - val_loss: 0.7171 - val_accuracy: 0.6176\n",
            "Epoch 33/600\n",
            "78/78 [==============================] - 0s 212us/sample - loss: 0.6314 - accuracy: 0.7308 - val_loss: 0.7129 - val_accuracy: 0.6176\n",
            "Epoch 34/600\n",
            "78/78 [==============================] - 0s 280us/sample - loss: 0.6268 - accuracy: 0.7308 - val_loss: 0.7088 - val_accuracy: 0.6176\n",
            "Epoch 35/600\n",
            "78/78 [==============================] - 0s 290us/sample - loss: 0.6224 - accuracy: 0.7308 - val_loss: 0.7049 - val_accuracy: 0.6176\n",
            "Epoch 36/600\n",
            "78/78 [==============================] - 0s 217us/sample - loss: 0.6181 - accuracy: 0.7308 - val_loss: 0.7012 - val_accuracy: 0.6176\n",
            "Epoch 37/600\n",
            "78/78 [==============================] - 0s 249us/sample - loss: 0.6140 - accuracy: 0.7308 - val_loss: 0.6975 - val_accuracy: 0.6176\n",
            "Epoch 38/600\n",
            "78/78 [==============================] - 0s 225us/sample - loss: 0.6101 - accuracy: 0.7308 - val_loss: 0.6939 - val_accuracy: 0.6176\n",
            "Epoch 39/600\n",
            "78/78 [==============================] - 0s 241us/sample - loss: 0.6063 - accuracy: 0.7308 - val_loss: 0.6905 - val_accuracy: 0.6176\n",
            "Epoch 40/600\n",
            "78/78 [==============================] - 0s 202us/sample - loss: 0.6026 - accuracy: 0.7308 - val_loss: 0.6871 - val_accuracy: 0.6176\n",
            "Epoch 41/600\n",
            "78/78 [==============================] - 0s 171us/sample - loss: 0.5993 - accuracy: 0.7308 - val_loss: 0.6840 - val_accuracy: 0.6176\n",
            "Epoch 42/600\n",
            "78/78 [==============================] - 0s 182us/sample - loss: 0.5961 - accuracy: 0.7308 - val_loss: 0.6810 - val_accuracy: 0.6176\n",
            "Epoch 43/600\n",
            "78/78 [==============================] - 0s 195us/sample - loss: 0.5931 - accuracy: 0.7308 - val_loss: 0.6781 - val_accuracy: 0.6176\n",
            "Epoch 44/600\n",
            "78/78 [==============================] - 0s 214us/sample - loss: 0.5901 - accuracy: 0.7308 - val_loss: 0.6753 - val_accuracy: 0.6176\n",
            "Epoch 45/600\n",
            "78/78 [==============================] - 0s 195us/sample - loss: 0.5873 - accuracy: 0.7308 - val_loss: 0.6725 - val_accuracy: 0.6176\n",
            "Epoch 46/600\n",
            "78/78 [==============================] - 0s 170us/sample - loss: 0.5845 - accuracy: 0.7308 - val_loss: 0.6698 - val_accuracy: 0.6176\n",
            "Epoch 47/600\n",
            "78/78 [==============================] - 0s 221us/sample - loss: 0.5818 - accuracy: 0.7308 - val_loss: 0.6672 - val_accuracy: 0.6176\n",
            "Epoch 48/600\n",
            "78/78 [==============================] - 0s 196us/sample - loss: 0.5791 - accuracy: 0.7308 - val_loss: 0.6647 - val_accuracy: 0.6176\n",
            "Epoch 49/600\n",
            "78/78 [==============================] - 0s 183us/sample - loss: 0.5765 - accuracy: 0.7308 - val_loss: 0.6621 - val_accuracy: 0.6176\n",
            "Epoch 50/600\n",
            "78/78 [==============================] - 0s 199us/sample - loss: 0.5740 - accuracy: 0.7308 - val_loss: 0.6596 - val_accuracy: 0.6176\n",
            "Epoch 51/600\n",
            "78/78 [==============================] - 0s 202us/sample - loss: 0.5716 - accuracy: 0.7308 - val_loss: 0.6572 - val_accuracy: 0.6176\n",
            "Epoch 52/600\n",
            "78/78 [==============================] - 0s 225us/sample - loss: 0.5692 - accuracy: 0.7308 - val_loss: 0.6548 - val_accuracy: 0.6176\n",
            "Epoch 53/600\n",
            "78/78 [==============================] - 0s 255us/sample - loss: 0.5669 - accuracy: 0.7308 - val_loss: 0.6525 - val_accuracy: 0.6176\n",
            "Epoch 54/600\n",
            "78/78 [==============================] - 0s 209us/sample - loss: 0.5646 - accuracy: 0.7308 - val_loss: 0.6502 - val_accuracy: 0.6176\n",
            "Epoch 55/600\n",
            "78/78 [==============================] - 0s 206us/sample - loss: 0.5624 - accuracy: 0.7308 - val_loss: 0.6479 - val_accuracy: 0.6176\n",
            "Epoch 56/600\n",
            "78/78 [==============================] - 0s 178us/sample - loss: 0.5602 - accuracy: 0.7308 - val_loss: 0.6456 - val_accuracy: 0.6176\n",
            "Epoch 57/600\n",
            "78/78 [==============================] - 0s 194us/sample - loss: 0.5581 - accuracy: 0.7308 - val_loss: 0.6434 - val_accuracy: 0.6176\n",
            "Epoch 58/600\n",
            "78/78 [==============================] - 0s 164us/sample - loss: 0.5560 - accuracy: 0.7308 - val_loss: 0.6413 - val_accuracy: 0.6176\n",
            "Epoch 59/600\n",
            "78/78 [==============================] - 0s 316us/sample - loss: 0.5540 - accuracy: 0.7308 - val_loss: 0.6392 - val_accuracy: 0.6176\n",
            "Epoch 60/600\n",
            "78/78 [==============================] - 0s 219us/sample - loss: 0.5520 - accuracy: 0.7308 - val_loss: 0.6371 - val_accuracy: 0.6176\n",
            "Epoch 61/600\n",
            "78/78 [==============================] - 0s 192us/sample - loss: 0.5501 - accuracy: 0.7308 - val_loss: 0.6351 - val_accuracy: 0.6176\n",
            "Epoch 62/600\n",
            "78/78 [==============================] - 0s 176us/sample - loss: 0.5482 - accuracy: 0.7308 - val_loss: 0.6330 - val_accuracy: 0.6176\n",
            "Epoch 63/600\n",
            "78/78 [==============================] - 0s 195us/sample - loss: 0.5463 - accuracy: 0.7308 - val_loss: 0.6310 - val_accuracy: 0.6176\n",
            "Epoch 64/600\n",
            "78/78 [==============================] - 0s 177us/sample - loss: 0.5445 - accuracy: 0.7308 - val_loss: 0.6290 - val_accuracy: 0.6176\n",
            "Epoch 65/600\n",
            "78/78 [==============================] - 0s 212us/sample - loss: 0.5427 - accuracy: 0.7308 - val_loss: 0.6271 - val_accuracy: 0.6176\n",
            "Epoch 66/600\n",
            "78/78 [==============================] - 0s 189us/sample - loss: 0.5409 - accuracy: 0.7308 - val_loss: 0.6252 - val_accuracy: 0.6176\n",
            "Epoch 67/600\n",
            "78/78 [==============================] - 0s 217us/sample - loss: 0.5392 - accuracy: 0.7308 - val_loss: 0.6233 - val_accuracy: 0.6176\n",
            "Epoch 68/600\n",
            "78/78 [==============================] - 0s 214us/sample - loss: 0.5375 - accuracy: 0.7308 - val_loss: 0.6214 - val_accuracy: 0.6176\n",
            "Epoch 69/600\n",
            "78/78 [==============================] - 0s 209us/sample - loss: 0.5358 - accuracy: 0.7308 - val_loss: 0.6196 - val_accuracy: 0.6176\n",
            "Epoch 70/600\n",
            "78/78 [==============================] - 0s 224us/sample - loss: 0.5342 - accuracy: 0.7308 - val_loss: 0.6177 - val_accuracy: 0.6176\n",
            "Epoch 71/600\n",
            "78/78 [==============================] - 0s 191us/sample - loss: 0.5326 - accuracy: 0.7308 - val_loss: 0.6160 - val_accuracy: 0.6176\n",
            "Epoch 72/600\n",
            "78/78 [==============================] - 0s 178us/sample - loss: 0.5310 - accuracy: 0.7308 - val_loss: 0.6142 - val_accuracy: 0.6176\n",
            "Epoch 73/600\n",
            "78/78 [==============================] - 0s 197us/sample - loss: 0.5294 - accuracy: 0.7308 - val_loss: 0.6125 - val_accuracy: 0.6176\n",
            "Epoch 74/600\n",
            "78/78 [==============================] - 0s 257us/sample - loss: 0.5278 - accuracy: 0.7308 - val_loss: 0.6108 - val_accuracy: 0.6176\n",
            "Epoch 75/600\n",
            "78/78 [==============================] - 0s 231us/sample - loss: 0.5263 - accuracy: 0.7308 - val_loss: 0.6091 - val_accuracy: 0.6176\n",
            "Epoch 76/600\n",
            "78/78 [==============================] - 0s 200us/sample - loss: 0.5248 - accuracy: 0.7308 - val_loss: 0.6074 - val_accuracy: 0.6176\n",
            "Epoch 77/600\n",
            "78/78 [==============================] - 0s 277us/sample - loss: 0.5233 - accuracy: 0.7308 - val_loss: 0.6057 - val_accuracy: 0.6176\n",
            "Epoch 78/600\n",
            "78/78 [==============================] - 0s 183us/sample - loss: 0.5218 - accuracy: 0.7308 - val_loss: 0.6041 - val_accuracy: 0.6176\n",
            "Epoch 79/600\n",
            "78/78 [==============================] - 0s 152us/sample - loss: 0.5203 - accuracy: 0.7308 - val_loss: 0.6024 - val_accuracy: 0.6176\n",
            "Epoch 80/600\n",
            "78/78 [==============================] - 0s 215us/sample - loss: 0.5189 - accuracy: 0.7308 - val_loss: 0.6009 - val_accuracy: 0.6176\n",
            "Epoch 81/600\n",
            "78/78 [==============================] - 0s 168us/sample - loss: 0.5175 - accuracy: 0.7308 - val_loss: 0.5993 - val_accuracy: 0.6176\n",
            "Epoch 82/600\n",
            "78/78 [==============================] - 0s 213us/sample - loss: 0.5161 - accuracy: 0.7308 - val_loss: 0.5977 - val_accuracy: 0.6176\n",
            "Epoch 83/600\n",
            "78/78 [==============================] - 0s 280us/sample - loss: 0.5147 - accuracy: 0.7308 - val_loss: 0.5962 - val_accuracy: 0.6176\n",
            "Epoch 84/600\n",
            "78/78 [==============================] - 0s 225us/sample - loss: 0.5133 - accuracy: 0.7308 - val_loss: 0.5947 - val_accuracy: 0.6176\n",
            "Epoch 85/600\n",
            "78/78 [==============================] - 0s 222us/sample - loss: 0.5120 - accuracy: 0.7308 - val_loss: 0.5932 - val_accuracy: 0.6176\n",
            "Epoch 86/600\n",
            "78/78 [==============================] - 0s 200us/sample - loss: 0.5107 - accuracy: 0.7308 - val_loss: 0.5917 - val_accuracy: 0.6176\n",
            "Epoch 87/600\n",
            "78/78 [==============================] - 0s 201us/sample - loss: 0.5094 - accuracy: 0.7308 - val_loss: 0.5902 - val_accuracy: 0.6176\n",
            "Epoch 88/600\n",
            "78/78 [==============================] - 0s 150us/sample - loss: 0.5081 - accuracy: 0.7308 - val_loss: 0.5888 - val_accuracy: 0.6176\n",
            "Epoch 89/600\n",
            "78/78 [==============================] - 0s 215us/sample - loss: 0.5068 - accuracy: 0.7308 - val_loss: 0.5873 - val_accuracy: 0.6176\n",
            "Epoch 90/600\n",
            "78/78 [==============================] - 0s 196us/sample - loss: 0.5055 - accuracy: 0.7308 - val_loss: 0.5859 - val_accuracy: 0.6176\n",
            "Epoch 91/600\n",
            "78/78 [==============================] - 0s 218us/sample - loss: 0.5043 - accuracy: 0.7308 - val_loss: 0.5845 - val_accuracy: 0.6176\n",
            "Epoch 92/600\n",
            "78/78 [==============================] - 0s 206us/sample - loss: 0.5030 - accuracy: 0.7308 - val_loss: 0.5830 - val_accuracy: 0.6176\n",
            "Epoch 93/600\n",
            "78/78 [==============================] - 0s 244us/sample - loss: 0.5018 - accuracy: 0.7308 - val_loss: 0.5817 - val_accuracy: 0.6176\n",
            "Epoch 94/600\n",
            "78/78 [==============================] - 0s 153us/sample - loss: 0.5006 - accuracy: 0.7308 - val_loss: 0.5803 - val_accuracy: 0.6176\n",
            "Epoch 95/600\n",
            "78/78 [==============================] - 0s 238us/sample - loss: 0.4994 - accuracy: 0.7308 - val_loss: 0.5789 - val_accuracy: 0.6176\n",
            "Epoch 96/600\n",
            "78/78 [==============================] - 0s 248us/sample - loss: 0.4982 - accuracy: 0.7308 - val_loss: 0.5776 - val_accuracy: 0.6176\n",
            "Epoch 97/600\n",
            "78/78 [==============================] - 0s 237us/sample - loss: 0.4970 - accuracy: 0.7308 - val_loss: 0.5763 - val_accuracy: 0.6176\n",
            "Epoch 98/600\n",
            "78/78 [==============================] - 0s 149us/sample - loss: 0.4958 - accuracy: 0.7436 - val_loss: 0.5749 - val_accuracy: 0.6176\n",
            "Epoch 99/600\n",
            "78/78 [==============================] - 0s 292us/sample - loss: 0.4947 - accuracy: 0.7436 - val_loss: 0.5736 - val_accuracy: 0.6176\n",
            "Epoch 100/600\n",
            "78/78 [==============================] - 0s 224us/sample - loss: 0.4936 - accuracy: 0.7436 - val_loss: 0.5723 - val_accuracy: 0.6176\n",
            "Epoch 101/600\n",
            "78/78 [==============================] - 0s 212us/sample - loss: 0.4924 - accuracy: 0.7436 - val_loss: 0.5709 - val_accuracy: 0.6176\n",
            "Epoch 102/600\n",
            "78/78 [==============================] - 0s 214us/sample - loss: 0.4913 - accuracy: 0.7436 - val_loss: 0.5697 - val_accuracy: 0.6176\n",
            "Epoch 103/600\n",
            "78/78 [==============================] - 0s 211us/sample - loss: 0.4902 - accuracy: 0.7436 - val_loss: 0.5684 - val_accuracy: 0.6176\n",
            "Epoch 104/600\n",
            "78/78 [==============================] - 0s 223us/sample - loss: 0.4892 - accuracy: 0.7436 - val_loss: 0.5671 - val_accuracy: 0.6176\n",
            "Epoch 105/600\n",
            "78/78 [==============================] - 0s 225us/sample - loss: 0.4881 - accuracy: 0.7436 - val_loss: 0.5659 - val_accuracy: 0.6176\n",
            "Epoch 106/600\n",
            "78/78 [==============================] - 0s 209us/sample - loss: 0.4870 - accuracy: 0.7436 - val_loss: 0.5646 - val_accuracy: 0.6176\n",
            "Epoch 107/600\n",
            "78/78 [==============================] - 0s 366us/sample - loss: 0.4859 - accuracy: 0.7436 - val_loss: 0.5634 - val_accuracy: 0.6176\n",
            "Epoch 108/600\n",
            "78/78 [==============================] - 0s 217us/sample - loss: 0.4849 - accuracy: 0.7436 - val_loss: 0.5622 - val_accuracy: 0.6176\n",
            "Epoch 109/600\n",
            "78/78 [==============================] - 0s 178us/sample - loss: 0.4838 - accuracy: 0.7436 - val_loss: 0.5609 - val_accuracy: 0.6176\n",
            "Epoch 110/600\n",
            "78/78 [==============================] - 0s 199us/sample - loss: 0.4828 - accuracy: 0.7436 - val_loss: 0.5596 - val_accuracy: 0.6176\n",
            "Epoch 111/600\n",
            "78/78 [==============================] - 0s 231us/sample - loss: 0.4817 - accuracy: 0.7436 - val_loss: 0.5583 - val_accuracy: 0.6176\n",
            "Epoch 112/600\n",
            "78/78 [==============================] - 0s 217us/sample - loss: 0.4806 - accuracy: 0.7436 - val_loss: 0.5571 - val_accuracy: 0.6176\n",
            "Epoch 113/600\n",
            "78/78 [==============================] - 0s 149us/sample - loss: 0.4796 - accuracy: 0.7436 - val_loss: 0.5558 - val_accuracy: 0.6176\n",
            "Epoch 114/600\n",
            "78/78 [==============================] - 0s 201us/sample - loss: 0.4785 - accuracy: 0.7436 - val_loss: 0.5545 - val_accuracy: 0.6471\n",
            "Epoch 115/600\n",
            "78/78 [==============================] - 0s 220us/sample - loss: 0.4775 - accuracy: 0.7436 - val_loss: 0.5532 - val_accuracy: 0.6471\n",
            "Epoch 116/600\n",
            "78/78 [==============================] - 0s 226us/sample - loss: 0.4764 - accuracy: 0.7436 - val_loss: 0.5518 - val_accuracy: 0.6471\n",
            "Epoch 117/600\n",
            "78/78 [==============================] - 0s 195us/sample - loss: 0.4753 - accuracy: 0.7436 - val_loss: 0.5504 - val_accuracy: 0.6471\n",
            "Epoch 118/600\n",
            "78/78 [==============================] - 0s 176us/sample - loss: 0.4742 - accuracy: 0.7436 - val_loss: 0.5488 - val_accuracy: 0.6471\n",
            "Epoch 119/600\n",
            "78/78 [==============================] - 0s 198us/sample - loss: 0.4730 - accuracy: 0.7436 - val_loss: 0.5471 - val_accuracy: 0.6471\n",
            "Epoch 120/600\n",
            "78/78 [==============================] - 0s 209us/sample - loss: 0.4717 - accuracy: 0.7436 - val_loss: 0.5450 - val_accuracy: 0.6471\n",
            "Epoch 121/600\n",
            "78/78 [==============================] - 0s 149us/sample - loss: 0.4702 - accuracy: 0.7436 - val_loss: 0.5429 - val_accuracy: 0.6471\n",
            "Epoch 122/600\n",
            "78/78 [==============================] - 0s 179us/sample - loss: 0.4684 - accuracy: 0.7436 - val_loss: 0.5403 - val_accuracy: 0.6471\n",
            "Epoch 123/600\n",
            "78/78 [==============================] - 0s 203us/sample - loss: 0.4664 - accuracy: 0.7436 - val_loss: 0.5376 - val_accuracy: 0.6471\n",
            "Epoch 124/600\n",
            "78/78 [==============================] - 0s 229us/sample - loss: 0.4639 - accuracy: 0.7436 - val_loss: 0.5348 - val_accuracy: 0.6471\n",
            "Epoch 125/600\n",
            "78/78 [==============================] - 0s 217us/sample - loss: 0.4614 - accuracy: 0.7436 - val_loss: 0.5321 - val_accuracy: 0.6471\n",
            "Epoch 126/600\n",
            "78/78 [==============================] - 0s 219us/sample - loss: 0.4590 - accuracy: 0.7564 - val_loss: 0.5298 - val_accuracy: 0.7059\n",
            "Epoch 127/600\n",
            "78/78 [==============================] - 0s 205us/sample - loss: 0.4572 - accuracy: 0.7564 - val_loss: 0.5283 - val_accuracy: 0.7059\n",
            "Epoch 128/600\n",
            "78/78 [==============================] - 0s 194us/sample - loss: 0.4558 - accuracy: 0.7564 - val_loss: 0.5271 - val_accuracy: 0.7059\n",
            "Epoch 129/600\n",
            "78/78 [==============================] - 0s 239us/sample - loss: 0.4546 - accuracy: 0.7692 - val_loss: 0.5259 - val_accuracy: 0.7059\n",
            "Epoch 130/600\n",
            "78/78 [==============================] - 0s 240us/sample - loss: 0.4534 - accuracy: 0.7692 - val_loss: 0.5250 - val_accuracy: 0.7059\n",
            "Epoch 131/600\n",
            "78/78 [==============================] - 0s 182us/sample - loss: 0.4525 - accuracy: 0.7692 - val_loss: 0.5241 - val_accuracy: 0.7059\n",
            "Epoch 132/600\n",
            "78/78 [==============================] - 0s 220us/sample - loss: 0.4516 - accuracy: 0.7692 - val_loss: 0.5233 - val_accuracy: 0.7059\n",
            "Epoch 133/600\n",
            "78/78 [==============================] - 0s 198us/sample - loss: 0.4507 - accuracy: 0.7692 - val_loss: 0.5225 - val_accuracy: 0.7059\n",
            "Epoch 134/600\n",
            "78/78 [==============================] - 0s 197us/sample - loss: 0.4499 - accuracy: 0.7692 - val_loss: 0.5217 - val_accuracy: 0.7059\n",
            "Epoch 135/600\n",
            "78/78 [==============================] - 0s 176us/sample - loss: 0.4490 - accuracy: 0.7692 - val_loss: 0.5209 - val_accuracy: 0.7059\n",
            "Epoch 136/600\n",
            "78/78 [==============================] - 0s 169us/sample - loss: 0.4482 - accuracy: 0.7692 - val_loss: 0.5201 - val_accuracy: 0.7059\n",
            "Epoch 137/600\n",
            "78/78 [==============================] - 0s 228us/sample - loss: 0.4474 - accuracy: 0.7692 - val_loss: 0.5192 - val_accuracy: 0.7059\n",
            "Epoch 138/600\n",
            "78/78 [==============================] - 0s 165us/sample - loss: 0.4466 - accuracy: 0.7692 - val_loss: 0.5184 - val_accuracy: 0.7059\n",
            "Epoch 139/600\n",
            "78/78 [==============================] - 0s 214us/sample - loss: 0.4457 - accuracy: 0.7692 - val_loss: 0.5175 - val_accuracy: 0.7059\n",
            "Epoch 140/600\n",
            "78/78 [==============================] - 0s 230us/sample - loss: 0.4449 - accuracy: 0.7692 - val_loss: 0.5167 - val_accuracy: 0.7059\n",
            "Epoch 141/600\n",
            "78/78 [==============================] - 0s 233us/sample - loss: 0.4441 - accuracy: 0.7692 - val_loss: 0.5159 - val_accuracy: 0.7059\n",
            "Epoch 142/600\n",
            "78/78 [==============================] - 0s 212us/sample - loss: 0.4434 - accuracy: 0.7692 - val_loss: 0.5150 - val_accuracy: 0.7059\n",
            "Epoch 143/600\n",
            "78/78 [==============================] - 0s 219us/sample - loss: 0.4426 - accuracy: 0.7692 - val_loss: 0.5142 - val_accuracy: 0.7059\n",
            "Epoch 144/600\n",
            "78/78 [==============================] - 0s 213us/sample - loss: 0.4418 - accuracy: 0.7692 - val_loss: 0.5133 - val_accuracy: 0.7059\n",
            "Epoch 145/600\n",
            "78/78 [==============================] - 0s 253us/sample - loss: 0.4410 - accuracy: 0.7692 - val_loss: 0.5126 - val_accuracy: 0.7059\n",
            "Epoch 146/600\n",
            "78/78 [==============================] - 0s 241us/sample - loss: 0.4403 - accuracy: 0.7692 - val_loss: 0.5116 - val_accuracy: 0.7059\n",
            "Epoch 147/600\n",
            "78/78 [==============================] - 0s 208us/sample - loss: 0.4395 - accuracy: 0.7692 - val_loss: 0.5108 - val_accuracy: 0.7059\n",
            "Epoch 148/600\n",
            "78/78 [==============================] - 0s 229us/sample - loss: 0.4387 - accuracy: 0.7821 - val_loss: 0.5100 - val_accuracy: 0.7059\n",
            "Epoch 149/600\n",
            "78/78 [==============================] - 0s 223us/sample - loss: 0.4379 - accuracy: 0.7821 - val_loss: 0.5092 - val_accuracy: 0.7059\n",
            "Epoch 150/600\n",
            "78/78 [==============================] - 0s 221us/sample - loss: 0.4372 - accuracy: 0.7821 - val_loss: 0.5082 - val_accuracy: 0.7059\n",
            "Epoch 151/600\n",
            "78/78 [==============================] - 0s 263us/sample - loss: 0.4365 - accuracy: 0.7821 - val_loss: 0.5074 - val_accuracy: 0.7059\n",
            "Epoch 152/600\n",
            "78/78 [==============================] - 0s 313us/sample - loss: 0.4357 - accuracy: 0.7821 - val_loss: 0.5065 - val_accuracy: 0.7059\n",
            "Epoch 153/600\n",
            "78/78 [==============================] - 0s 211us/sample - loss: 0.4350 - accuracy: 0.7821 - val_loss: 0.5058 - val_accuracy: 0.7059\n",
            "Epoch 154/600\n",
            "78/78 [==============================] - 0s 226us/sample - loss: 0.4342 - accuracy: 0.7821 - val_loss: 0.5048 - val_accuracy: 0.7353\n",
            "Epoch 155/600\n",
            "78/78 [==============================] - 0s 236us/sample - loss: 0.4335 - accuracy: 0.7821 - val_loss: 0.5041 - val_accuracy: 0.7353\n",
            "Epoch 156/600\n",
            "78/78 [==============================] - 0s 171us/sample - loss: 0.4328 - accuracy: 0.7821 - val_loss: 0.5031 - val_accuracy: 0.7353\n",
            "Epoch 157/600\n",
            "78/78 [==============================] - 0s 238us/sample - loss: 0.4320 - accuracy: 0.7821 - val_loss: 0.5023 - val_accuracy: 0.7353\n",
            "Epoch 158/600\n",
            "78/78 [==============================] - 0s 179us/sample - loss: 0.4313 - accuracy: 0.7821 - val_loss: 0.5014 - val_accuracy: 0.7353\n",
            "Epoch 159/600\n",
            "78/78 [==============================] - 0s 207us/sample - loss: 0.4306 - accuracy: 0.7821 - val_loss: 0.5007 - val_accuracy: 0.7353\n",
            "Epoch 160/600\n",
            "78/78 [==============================] - 0s 184us/sample - loss: 0.4299 - accuracy: 0.7949 - val_loss: 0.4998 - val_accuracy: 0.7353\n",
            "Epoch 161/600\n",
            "78/78 [==============================] - 0s 222us/sample - loss: 0.4292 - accuracy: 0.7949 - val_loss: 0.4989 - val_accuracy: 0.7353\n",
            "Epoch 162/600\n",
            "78/78 [==============================] - 0s 212us/sample - loss: 0.4285 - accuracy: 0.7949 - val_loss: 0.4982 - val_accuracy: 0.7353\n",
            "Epoch 163/600\n",
            "78/78 [==============================] - 0s 158us/sample - loss: 0.4277 - accuracy: 0.7949 - val_loss: 0.4973 - val_accuracy: 0.7353\n",
            "Epoch 164/600\n",
            "78/78 [==============================] - 0s 188us/sample - loss: 0.4270 - accuracy: 0.7949 - val_loss: 0.4965 - val_accuracy: 0.7353\n",
            "Epoch 165/600\n",
            "78/78 [==============================] - 0s 222us/sample - loss: 0.4263 - accuracy: 0.7949 - val_loss: 0.4957 - val_accuracy: 0.7353\n",
            "Epoch 166/600\n",
            "78/78 [==============================] - 0s 230us/sample - loss: 0.4256 - accuracy: 0.7949 - val_loss: 0.4948 - val_accuracy: 0.7353\n",
            "Epoch 167/600\n",
            "78/78 [==============================] - 0s 244us/sample - loss: 0.4250 - accuracy: 0.7949 - val_loss: 0.4941 - val_accuracy: 0.7353\n",
            "Epoch 168/600\n",
            "78/78 [==============================] - 0s 196us/sample - loss: 0.4243 - accuracy: 0.7949 - val_loss: 0.4932 - val_accuracy: 0.7353\n",
            "Epoch 169/600\n",
            "78/78 [==============================] - 0s 219us/sample - loss: 0.4236 - accuracy: 0.7949 - val_loss: 0.4925 - val_accuracy: 0.7353\n",
            "Epoch 170/600\n",
            "78/78 [==============================] - 0s 188us/sample - loss: 0.4229 - accuracy: 0.7949 - val_loss: 0.4916 - val_accuracy: 0.7353\n",
            "Epoch 171/600\n",
            "78/78 [==============================] - 0s 201us/sample - loss: 0.4222 - accuracy: 0.7949 - val_loss: 0.4909 - val_accuracy: 0.7353\n",
            "Epoch 172/600\n",
            "78/78 [==============================] - 0s 185us/sample - loss: 0.4215 - accuracy: 0.7949 - val_loss: 0.4901 - val_accuracy: 0.7353\n",
            "Epoch 173/600\n",
            "78/78 [==============================] - 0s 171us/sample - loss: 0.4209 - accuracy: 0.7949 - val_loss: 0.4893 - val_accuracy: 0.7353\n",
            "Epoch 174/600\n",
            "78/78 [==============================] - 0s 217us/sample - loss: 0.4202 - accuracy: 0.7949 - val_loss: 0.4885 - val_accuracy: 0.7353\n",
            "Epoch 175/600\n",
            "78/78 [==============================] - 0s 225us/sample - loss: 0.4195 - accuracy: 0.7949 - val_loss: 0.4877 - val_accuracy: 0.7353\n",
            "Epoch 176/600\n",
            "78/78 [==============================] - 0s 197us/sample - loss: 0.4189 - accuracy: 0.7949 - val_loss: 0.4870 - val_accuracy: 0.7353\n",
            "Epoch 177/600\n",
            "78/78 [==============================] - 0s 197us/sample - loss: 0.4182 - accuracy: 0.7949 - val_loss: 0.4862 - val_accuracy: 0.7353\n",
            "Epoch 178/600\n",
            "78/78 [==============================] - 0s 206us/sample - loss: 0.4175 - accuracy: 0.7949 - val_loss: 0.4854 - val_accuracy: 0.7353\n",
            "Epoch 179/600\n",
            "78/78 [==============================] - 0s 213us/sample - loss: 0.4169 - accuracy: 0.7949 - val_loss: 0.4847 - val_accuracy: 0.7353\n",
            "Epoch 180/600\n",
            "78/78 [==============================] - 0s 229us/sample - loss: 0.4162 - accuracy: 0.7949 - val_loss: 0.4839 - val_accuracy: 0.7353\n",
            "Epoch 181/600\n",
            "78/78 [==============================] - 0s 251us/sample - loss: 0.4156 - accuracy: 0.7949 - val_loss: 0.4832 - val_accuracy: 0.7353\n",
            "Epoch 182/600\n",
            "78/78 [==============================] - 0s 188us/sample - loss: 0.4149 - accuracy: 0.7949 - val_loss: 0.4824 - val_accuracy: 0.7647\n",
            "Epoch 183/600\n",
            "78/78 [==============================] - 0s 212us/sample - loss: 0.4143 - accuracy: 0.7949 - val_loss: 0.4817 - val_accuracy: 0.7647\n",
            "Epoch 184/600\n",
            "78/78 [==============================] - 0s 229us/sample - loss: 0.4136 - accuracy: 0.7949 - val_loss: 0.4809 - val_accuracy: 0.7647\n",
            "Epoch 185/600\n",
            "78/78 [==============================] - 0s 200us/sample - loss: 0.4130 - accuracy: 0.7949 - val_loss: 0.4801 - val_accuracy: 0.7647\n",
            "Epoch 186/600\n",
            "78/78 [==============================] - 0s 194us/sample - loss: 0.4124 - accuracy: 0.7949 - val_loss: 0.4794 - val_accuracy: 0.7647\n",
            "Epoch 187/600\n",
            "78/78 [==============================] - 0s 209us/sample - loss: 0.4117 - accuracy: 0.7949 - val_loss: 0.4787 - val_accuracy: 0.7647\n",
            "Epoch 188/600\n",
            "78/78 [==============================] - 0s 206us/sample - loss: 0.4111 - accuracy: 0.7949 - val_loss: 0.4779 - val_accuracy: 0.7647\n",
            "Epoch 189/600\n",
            "78/78 [==============================] - 0s 162us/sample - loss: 0.4105 - accuracy: 0.7949 - val_loss: 0.4772 - val_accuracy: 0.7647\n",
            "Epoch 190/600\n",
            "78/78 [==============================] - 0s 245us/sample - loss: 0.4099 - accuracy: 0.7949 - val_loss: 0.4764 - val_accuracy: 0.7647\n",
            "Epoch 191/600\n",
            "78/78 [==============================] - 0s 214us/sample - loss: 0.4092 - accuracy: 0.8077 - val_loss: 0.4757 - val_accuracy: 0.7647\n",
            "Epoch 192/600\n",
            "78/78 [==============================] - 0s 224us/sample - loss: 0.4086 - accuracy: 0.8077 - val_loss: 0.4749 - val_accuracy: 0.7647\n",
            "Epoch 193/600\n",
            "78/78 [==============================] - 0s 196us/sample - loss: 0.4080 - accuracy: 0.8077 - val_loss: 0.4742 - val_accuracy: 0.7647\n",
            "Epoch 194/600\n",
            "78/78 [==============================] - 0s 207us/sample - loss: 0.4074 - accuracy: 0.8077 - val_loss: 0.4735 - val_accuracy: 0.7647\n",
            "Epoch 195/600\n",
            "78/78 [==============================] - 0s 208us/sample - loss: 0.4068 - accuracy: 0.8077 - val_loss: 0.4728 - val_accuracy: 0.7647\n",
            "Epoch 196/600\n",
            "78/78 [==============================] - 0s 235us/sample - loss: 0.4062 - accuracy: 0.8077 - val_loss: 0.4720 - val_accuracy: 0.7647\n",
            "Epoch 197/600\n",
            "78/78 [==============================] - 0s 163us/sample - loss: 0.4056 - accuracy: 0.8077 - val_loss: 0.4714 - val_accuracy: 0.7647\n",
            "Epoch 198/600\n",
            "78/78 [==============================] - 0s 206us/sample - loss: 0.4050 - accuracy: 0.8077 - val_loss: 0.4706 - val_accuracy: 0.7647\n",
            "Epoch 199/600\n",
            "78/78 [==============================] - 0s 209us/sample - loss: 0.4043 - accuracy: 0.8077 - val_loss: 0.4700 - val_accuracy: 0.7647\n",
            "Epoch 200/600\n",
            "78/78 [==============================] - 0s 206us/sample - loss: 0.4037 - accuracy: 0.8077 - val_loss: 0.4693 - val_accuracy: 0.7647\n",
            "Epoch 201/600\n",
            "78/78 [==============================] - 0s 184us/sample - loss: 0.4031 - accuracy: 0.8077 - val_loss: 0.4686 - val_accuracy: 0.7941\n",
            "Epoch 202/600\n",
            "78/78 [==============================] - 0s 208us/sample - loss: 0.4025 - accuracy: 0.8077 - val_loss: 0.4679 - val_accuracy: 0.7941\n",
            "Epoch 203/600\n",
            "78/78 [==============================] - 0s 218us/sample - loss: 0.4019 - accuracy: 0.8077 - val_loss: 0.4672 - val_accuracy: 0.7941\n",
            "Epoch 204/600\n",
            "78/78 [==============================] - 0s 186us/sample - loss: 0.4013 - accuracy: 0.8077 - val_loss: 0.4665 - val_accuracy: 0.7941\n",
            "Epoch 205/600\n",
            "78/78 [==============================] - 0s 210us/sample - loss: 0.4007 - accuracy: 0.8077 - val_loss: 0.4658 - val_accuracy: 0.8235\n",
            "Epoch 206/600\n",
            "78/78 [==============================] - 0s 242us/sample - loss: 0.4001 - accuracy: 0.8077 - val_loss: 0.4651 - val_accuracy: 0.8235\n",
            "Epoch 207/600\n",
            "78/78 [==============================] - 0s 172us/sample - loss: 0.3996 - accuracy: 0.8205 - val_loss: 0.4644 - val_accuracy: 0.8235\n",
            "Epoch 208/600\n",
            "78/78 [==============================] - 0s 214us/sample - loss: 0.3990 - accuracy: 0.8205 - val_loss: 0.4637 - val_accuracy: 0.8235\n",
            "Epoch 209/600\n",
            "78/78 [==============================] - 0s 238us/sample - loss: 0.3984 - accuracy: 0.8205 - val_loss: 0.4631 - val_accuracy: 0.8235\n",
            "Epoch 210/600\n",
            "78/78 [==============================] - 0s 214us/sample - loss: 0.3978 - accuracy: 0.8205 - val_loss: 0.4624 - val_accuracy: 0.8235\n",
            "Epoch 211/600\n",
            "78/78 [==============================] - 0s 263us/sample - loss: 0.3972 - accuracy: 0.8205 - val_loss: 0.4616 - val_accuracy: 0.8235\n",
            "Epoch 212/600\n",
            "78/78 [==============================] - 0s 212us/sample - loss: 0.3966 - accuracy: 0.8205 - val_loss: 0.4609 - val_accuracy: 0.8235\n",
            "Epoch 213/600\n",
            "78/78 [==============================] - 0s 220us/sample - loss: 0.3960 - accuracy: 0.8205 - val_loss: 0.4602 - val_accuracy: 0.8235\n",
            "Epoch 214/600\n",
            "78/78 [==============================] - 0s 261us/sample - loss: 0.3954 - accuracy: 0.8333 - val_loss: 0.4594 - val_accuracy: 0.8235\n",
            "Epoch 215/600\n",
            "78/78 [==============================] - 0s 171us/sample - loss: 0.3948 - accuracy: 0.8333 - val_loss: 0.4588 - val_accuracy: 0.8235\n",
            "Epoch 216/600\n",
            "78/78 [==============================] - 0s 194us/sample - loss: 0.3942 - accuracy: 0.8333 - val_loss: 0.4580 - val_accuracy: 0.8235\n",
            "Epoch 217/600\n",
            "78/78 [==============================] - 0s 242us/sample - loss: 0.3936 - accuracy: 0.8333 - val_loss: 0.4573 - val_accuracy: 0.8235\n",
            "Epoch 218/600\n",
            "78/78 [==============================] - 0s 239us/sample - loss: 0.3930 - accuracy: 0.8333 - val_loss: 0.4566 - val_accuracy: 0.8235\n",
            "Epoch 219/600\n",
            "78/78 [==============================] - 0s 255us/sample - loss: 0.3924 - accuracy: 0.8333 - val_loss: 0.4558 - val_accuracy: 0.8235\n",
            "Epoch 220/600\n",
            "78/78 [==============================] - 0s 199us/sample - loss: 0.3918 - accuracy: 0.8333 - val_loss: 0.4550 - val_accuracy: 0.8235\n",
            "Epoch 221/600\n",
            "78/78 [==============================] - 0s 246us/sample - loss: 0.3912 - accuracy: 0.8462 - val_loss: 0.4543 - val_accuracy: 0.8235\n",
            "Epoch 222/600\n",
            "78/78 [==============================] - 0s 211us/sample - loss: 0.3906 - accuracy: 0.8462 - val_loss: 0.4536 - val_accuracy: 0.8235\n",
            "Epoch 223/600\n",
            "78/78 [==============================] - 0s 245us/sample - loss: 0.3900 - accuracy: 0.8462 - val_loss: 0.4528 - val_accuracy: 0.8235\n",
            "Epoch 224/600\n",
            "78/78 [==============================] - 0s 257us/sample - loss: 0.3893 - accuracy: 0.8462 - val_loss: 0.4520 - val_accuracy: 0.8235\n",
            "Epoch 225/600\n",
            "78/78 [==============================] - 0s 260us/sample - loss: 0.3887 - accuracy: 0.8462 - val_loss: 0.4511 - val_accuracy: 0.8235\n",
            "Epoch 226/600\n",
            "78/78 [==============================] - 0s 172us/sample - loss: 0.3880 - accuracy: 0.8462 - val_loss: 0.4501 - val_accuracy: 0.8235\n",
            "Epoch 227/600\n",
            "78/78 [==============================] - 0s 188us/sample - loss: 0.3872 - accuracy: 0.8462 - val_loss: 0.4490 - val_accuracy: 0.8235\n",
            "Epoch 228/600\n",
            "78/78 [==============================] - 0s 207us/sample - loss: 0.3863 - accuracy: 0.8462 - val_loss: 0.4478 - val_accuracy: 0.8235\n",
            "Epoch 229/600\n",
            "78/78 [==============================] - 0s 255us/sample - loss: 0.3853 - accuracy: 0.8590 - val_loss: 0.4465 - val_accuracy: 0.8529\n",
            "Epoch 230/600\n",
            "78/78 [==============================] - 0s 213us/sample - loss: 0.3843 - accuracy: 0.8718 - val_loss: 0.4452 - val_accuracy: 0.8529\n",
            "Epoch 231/600\n",
            "78/78 [==============================] - 0s 227us/sample - loss: 0.3834 - accuracy: 0.8718 - val_loss: 0.4443 - val_accuracy: 0.8529\n",
            "Epoch 232/600\n",
            "78/78 [==============================] - 0s 182us/sample - loss: 0.3826 - accuracy: 0.8718 - val_loss: 0.4433 - val_accuracy: 0.8529\n",
            "Epoch 233/600\n",
            "78/78 [==============================] - 0s 212us/sample - loss: 0.3819 - accuracy: 0.8718 - val_loss: 0.4424 - val_accuracy: 0.8529\n",
            "Epoch 234/600\n",
            "78/78 [==============================] - 0s 181us/sample - loss: 0.3813 - accuracy: 0.8718 - val_loss: 0.4417 - val_accuracy: 0.8529\n",
            "Epoch 235/600\n",
            "78/78 [==============================] - 0s 213us/sample - loss: 0.3807 - accuracy: 0.8718 - val_loss: 0.4408 - val_accuracy: 0.8529\n",
            "Epoch 236/600\n",
            "78/78 [==============================] - 0s 193us/sample - loss: 0.3801 - accuracy: 0.8718 - val_loss: 0.4400 - val_accuracy: 0.8529\n",
            "Epoch 237/600\n",
            "78/78 [==============================] - 0s 209us/sample - loss: 0.3794 - accuracy: 0.8718 - val_loss: 0.4391 - val_accuracy: 0.8529\n",
            "Epoch 238/600\n",
            "78/78 [==============================] - 0s 184us/sample - loss: 0.3788 - accuracy: 0.8718 - val_loss: 0.4383 - val_accuracy: 0.8529\n",
            "Epoch 239/600\n",
            "78/78 [==============================] - 0s 246us/sample - loss: 0.3782 - accuracy: 0.8718 - val_loss: 0.4375 - val_accuracy: 0.8529\n",
            "Epoch 240/600\n",
            "78/78 [==============================] - 0s 192us/sample - loss: 0.3776 - accuracy: 0.8718 - val_loss: 0.4366 - val_accuracy: 0.8529\n",
            "Epoch 241/600\n",
            "78/78 [==============================] - 0s 295us/sample - loss: 0.3770 - accuracy: 0.8718 - val_loss: 0.4358 - val_accuracy: 0.8529\n",
            "Epoch 242/600\n",
            "78/78 [==============================] - 0s 206us/sample - loss: 0.3764 - accuracy: 0.8718 - val_loss: 0.4350 - val_accuracy: 0.8529\n",
            "Epoch 243/600\n",
            "78/78 [==============================] - 0s 245us/sample - loss: 0.3758 - accuracy: 0.8718 - val_loss: 0.4343 - val_accuracy: 0.8529\n",
            "Epoch 244/600\n",
            "78/78 [==============================] - 0s 202us/sample - loss: 0.3752 - accuracy: 0.8718 - val_loss: 0.4336 - val_accuracy: 0.8529\n",
            "Epoch 245/600\n",
            "78/78 [==============================] - 0s 186us/sample - loss: 0.3747 - accuracy: 0.8718 - val_loss: 0.4330 - val_accuracy: 0.8529\n",
            "Epoch 246/600\n",
            "78/78 [==============================] - 0s 229us/sample - loss: 0.3741 - accuracy: 0.8718 - val_loss: 0.4323 - val_accuracy: 0.8529\n",
            "Epoch 247/600\n",
            "78/78 [==============================] - 0s 230us/sample - loss: 0.3736 - accuracy: 0.8718 - val_loss: 0.4317 - val_accuracy: 0.8529\n",
            "Epoch 248/600\n",
            "78/78 [==============================] - 0s 183us/sample - loss: 0.3731 - accuracy: 0.8718 - val_loss: 0.4312 - val_accuracy: 0.8529\n",
            "Epoch 249/600\n",
            "78/78 [==============================] - 0s 196us/sample - loss: 0.3725 - accuracy: 0.8718 - val_loss: 0.4306 - val_accuracy: 0.8529\n",
            "Epoch 250/600\n",
            "78/78 [==============================] - 0s 230us/sample - loss: 0.3720 - accuracy: 0.8718 - val_loss: 0.4300 - val_accuracy: 0.8529\n",
            "Epoch 251/600\n",
            "78/78 [==============================] - 0s 165us/sample - loss: 0.3715 - accuracy: 0.8718 - val_loss: 0.4294 - val_accuracy: 0.8529\n",
            "Epoch 252/600\n",
            "78/78 [==============================] - 0s 174us/sample - loss: 0.3710 - accuracy: 0.8718 - val_loss: 0.4288 - val_accuracy: 0.8529\n",
            "Epoch 253/600\n",
            "78/78 [==============================] - 0s 210us/sample - loss: 0.3705 - accuracy: 0.8846 - val_loss: 0.4282 - val_accuracy: 0.8529\n",
            "Epoch 254/600\n",
            "78/78 [==============================] - 0s 233us/sample - loss: 0.3700 - accuracy: 0.8846 - val_loss: 0.4276 - val_accuracy: 0.8529\n",
            "Epoch 255/600\n",
            "78/78 [==============================] - 0s 232us/sample - loss: 0.3694 - accuracy: 0.8846 - val_loss: 0.4270 - val_accuracy: 0.8529\n",
            "Epoch 256/600\n",
            "78/78 [==============================] - 0s 197us/sample - loss: 0.3689 - accuracy: 0.8846 - val_loss: 0.4264 - val_accuracy: 0.8824\n",
            "Epoch 257/600\n",
            "78/78 [==============================] - 0s 147us/sample - loss: 0.3684 - accuracy: 0.8846 - val_loss: 0.4258 - val_accuracy: 0.8824\n",
            "Epoch 258/600\n",
            "78/78 [==============================] - 0s 155us/sample - loss: 0.3679 - accuracy: 0.8846 - val_loss: 0.4252 - val_accuracy: 0.8824\n",
            "Epoch 259/600\n",
            "78/78 [==============================] - 0s 206us/sample - loss: 0.3674 - accuracy: 0.8846 - val_loss: 0.4246 - val_accuracy: 0.8824\n",
            "Epoch 260/600\n",
            "78/78 [==============================] - 0s 161us/sample - loss: 0.3669 - accuracy: 0.8846 - val_loss: 0.4240 - val_accuracy: 0.8824\n",
            "Epoch 261/600\n",
            "78/78 [==============================] - 0s 237us/sample - loss: 0.3664 - accuracy: 0.8846 - val_loss: 0.4234 - val_accuracy: 0.8824\n",
            "Epoch 262/600\n",
            "78/78 [==============================] - 0s 191us/sample - loss: 0.3659 - accuracy: 0.8974 - val_loss: 0.4228 - val_accuracy: 0.8824\n",
            "Epoch 263/600\n",
            "78/78 [==============================] - 0s 205us/sample - loss: 0.3654 - accuracy: 0.8974 - val_loss: 0.4222 - val_accuracy: 0.8824\n",
            "Epoch 264/600\n",
            "78/78 [==============================] - 0s 187us/sample - loss: 0.3649 - accuracy: 0.8974 - val_loss: 0.4217 - val_accuracy: 0.8824\n",
            "Epoch 265/600\n",
            "78/78 [==============================] - 0s 197us/sample - loss: 0.3644 - accuracy: 0.8974 - val_loss: 0.4211 - val_accuracy: 0.8824\n",
            "Epoch 266/600\n",
            "78/78 [==============================] - 0s 207us/sample - loss: 0.3639 - accuracy: 0.8974 - val_loss: 0.4205 - val_accuracy: 0.8824\n",
            "Epoch 267/600\n",
            "78/78 [==============================] - 0s 189us/sample - loss: 0.3634 - accuracy: 0.8974 - val_loss: 0.4199 - val_accuracy: 0.8824\n",
            "Epoch 268/600\n",
            "78/78 [==============================] - 0s 206us/sample - loss: 0.3629 - accuracy: 0.8974 - val_loss: 0.4194 - val_accuracy: 0.8824\n",
            "Epoch 269/600\n",
            "78/78 [==============================] - 0s 148us/sample - loss: 0.3624 - accuracy: 0.8974 - val_loss: 0.4188 - val_accuracy: 0.8824\n",
            "Epoch 270/600\n",
            "78/78 [==============================] - 0s 224us/sample - loss: 0.3620 - accuracy: 0.8974 - val_loss: 0.4182 - val_accuracy: 0.8824\n",
            "Epoch 271/600\n",
            "78/78 [==============================] - 0s 188us/sample - loss: 0.3615 - accuracy: 0.8974 - val_loss: 0.4176 - val_accuracy: 0.8824\n",
            "Epoch 272/600\n",
            "78/78 [==============================] - 0s 226us/sample - loss: 0.3610 - accuracy: 0.8974 - val_loss: 0.4170 - val_accuracy: 0.8824\n",
            "Epoch 273/600\n",
            "78/78 [==============================] - 0s 165us/sample - loss: 0.3605 - accuracy: 0.8974 - val_loss: 0.4164 - val_accuracy: 0.8824\n",
            "Epoch 274/600\n",
            "78/78 [==============================] - 0s 186us/sample - loss: 0.3600 - accuracy: 0.8974 - val_loss: 0.4159 - val_accuracy: 0.8824\n",
            "Epoch 275/600\n",
            "78/78 [==============================] - 0s 192us/sample - loss: 0.3596 - accuracy: 0.8974 - val_loss: 0.4153 - val_accuracy: 0.8824\n",
            "Epoch 276/600\n",
            "78/78 [==============================] - 0s 145us/sample - loss: 0.3591 - accuracy: 0.8974 - val_loss: 0.4147 - val_accuracy: 0.8824\n",
            "Epoch 277/600\n",
            "78/78 [==============================] - 0s 189us/sample - loss: 0.3586 - accuracy: 0.8974 - val_loss: 0.4141 - val_accuracy: 0.8824\n",
            "Epoch 278/600\n",
            "78/78 [==============================] - 0s 200us/sample - loss: 0.3581 - accuracy: 0.8974 - val_loss: 0.4135 - val_accuracy: 0.8824\n",
            "Epoch 279/600\n",
            "78/78 [==============================] - 0s 204us/sample - loss: 0.3576 - accuracy: 0.8974 - val_loss: 0.4130 - val_accuracy: 0.8824\n",
            "Epoch 280/600\n",
            "78/78 [==============================] - 0s 204us/sample - loss: 0.3572 - accuracy: 0.8974 - val_loss: 0.4124 - val_accuracy: 0.8824\n",
            "Epoch 281/600\n",
            "78/78 [==============================] - 0s 191us/sample - loss: 0.3567 - accuracy: 0.8974 - val_loss: 0.4118 - val_accuracy: 0.9118\n",
            "Epoch 282/600\n",
            "78/78 [==============================] - 0s 141us/sample - loss: 0.3562 - accuracy: 0.8974 - val_loss: 0.4113 - val_accuracy: 0.9118\n",
            "Epoch 283/600\n",
            "78/78 [==============================] - 0s 214us/sample - loss: 0.3558 - accuracy: 0.8974 - val_loss: 0.4107 - val_accuracy: 0.9118\n",
            "Epoch 284/600\n",
            "78/78 [==============================] - 0s 161us/sample - loss: 0.3553 - accuracy: 0.8974 - val_loss: 0.4101 - val_accuracy: 0.9118\n",
            "Epoch 285/600\n",
            "78/78 [==============================] - 0s 216us/sample - loss: 0.3548 - accuracy: 0.8974 - val_loss: 0.4096 - val_accuracy: 0.9118\n",
            "Epoch 286/600\n",
            "78/78 [==============================] - 0s 216us/sample - loss: 0.3544 - accuracy: 0.8974 - val_loss: 0.4090 - val_accuracy: 0.9118\n",
            "Epoch 287/600\n",
            "78/78 [==============================] - 0s 188us/sample - loss: 0.3539 - accuracy: 0.8974 - val_loss: 0.4085 - val_accuracy: 0.9118\n",
            "Epoch 288/600\n",
            "78/78 [==============================] - 0s 204us/sample - loss: 0.3534 - accuracy: 0.8974 - val_loss: 0.4079 - val_accuracy: 0.9118\n",
            "Epoch 289/600\n",
            "78/78 [==============================] - 0s 168us/sample - loss: 0.3530 - accuracy: 0.8974 - val_loss: 0.4074 - val_accuracy: 0.9118\n",
            "Epoch 290/600\n",
            "78/78 [==============================] - 0s 140us/sample - loss: 0.3525 - accuracy: 0.8974 - val_loss: 0.4068 - val_accuracy: 0.9118\n",
            "Epoch 291/600\n",
            "78/78 [==============================] - 0s 210us/sample - loss: 0.3520 - accuracy: 0.8974 - val_loss: 0.4062 - val_accuracy: 0.9118\n",
            "Epoch 292/600\n",
            "78/78 [==============================] - 0s 267us/sample - loss: 0.3516 - accuracy: 0.8974 - val_loss: 0.4057 - val_accuracy: 0.9118\n",
            "Epoch 293/600\n",
            "78/78 [==============================] - 0s 220us/sample - loss: 0.3511 - accuracy: 0.8974 - val_loss: 0.4052 - val_accuracy: 0.9118\n",
            "Epoch 294/600\n",
            "78/78 [==============================] - 0s 144us/sample - loss: 0.3507 - accuracy: 0.8974 - val_loss: 0.4046 - val_accuracy: 0.9118\n",
            "Epoch 295/600\n",
            "78/78 [==============================] - 0s 229us/sample - loss: 0.3502 - accuracy: 0.8974 - val_loss: 0.4041 - val_accuracy: 0.9118\n",
            "Epoch 296/600\n",
            "78/78 [==============================] - 0s 198us/sample - loss: 0.3497 - accuracy: 0.8974 - val_loss: 0.4035 - val_accuracy: 0.9118\n",
            "Epoch 297/600\n",
            "78/78 [==============================] - 0s 216us/sample - loss: 0.3493 - accuracy: 0.8974 - val_loss: 0.4030 - val_accuracy: 0.9118\n",
            "Epoch 298/600\n",
            "78/78 [==============================] - 0s 182us/sample - loss: 0.3488 - accuracy: 0.8974 - val_loss: 0.4025 - val_accuracy: 0.9118\n",
            "Epoch 299/600\n",
            "78/78 [==============================] - 0s 247us/sample - loss: 0.3484 - accuracy: 0.8974 - val_loss: 0.4019 - val_accuracy: 0.9118\n",
            "Epoch 300/600\n",
            "78/78 [==============================] - 0s 194us/sample - loss: 0.3479 - accuracy: 0.9103 - val_loss: 0.4014 - val_accuracy: 0.9118\n",
            "Epoch 301/600\n",
            "78/78 [==============================] - 0s 232us/sample - loss: 0.3475 - accuracy: 0.9103 - val_loss: 0.4009 - val_accuracy: 0.9118\n",
            "Epoch 302/600\n",
            "78/78 [==============================] - 0s 186us/sample - loss: 0.3470 - accuracy: 0.9103 - val_loss: 0.4003 - val_accuracy: 0.9118\n",
            "Epoch 303/600\n",
            "78/78 [==============================] - 0s 189us/sample - loss: 0.3466 - accuracy: 0.9103 - val_loss: 0.3998 - val_accuracy: 0.9118\n",
            "Epoch 304/600\n",
            "78/78 [==============================] - 0s 193us/sample - loss: 0.3461 - accuracy: 0.9103 - val_loss: 0.3993 - val_accuracy: 0.9118\n",
            "Epoch 305/600\n",
            "78/78 [==============================] - 0s 184us/sample - loss: 0.3457 - accuracy: 0.9103 - val_loss: 0.3987 - val_accuracy: 0.9118\n",
            "Epoch 306/600\n",
            "78/78 [==============================] - 0s 181us/sample - loss: 0.3452 - accuracy: 0.9103 - val_loss: 0.3982 - val_accuracy: 0.9118\n",
            "Epoch 307/600\n",
            "78/78 [==============================] - 0s 210us/sample - loss: 0.3448 - accuracy: 0.9231 - val_loss: 0.3977 - val_accuracy: 0.9118\n",
            "Epoch 308/600\n",
            "78/78 [==============================] - 0s 172us/sample - loss: 0.3443 - accuracy: 0.9231 - val_loss: 0.3972 - val_accuracy: 0.9118\n",
            "Epoch 309/600\n",
            "78/78 [==============================] - 0s 188us/sample - loss: 0.3439 - accuracy: 0.9231 - val_loss: 0.3967 - val_accuracy: 0.9118\n",
            "Epoch 310/600\n",
            "78/78 [==============================] - 0s 189us/sample - loss: 0.3435 - accuracy: 0.9231 - val_loss: 0.3962 - val_accuracy: 0.9118\n",
            "Epoch 311/600\n",
            "78/78 [==============================] - 0s 147us/sample - loss: 0.3430 - accuracy: 0.9231 - val_loss: 0.3957 - val_accuracy: 0.9118\n",
            "Epoch 312/600\n",
            "78/78 [==============================] - 0s 196us/sample - loss: 0.3426 - accuracy: 0.9231 - val_loss: 0.3952 - val_accuracy: 0.9118\n",
            "Epoch 313/600\n",
            "78/78 [==============================] - 0s 182us/sample - loss: 0.3421 - accuracy: 0.9231 - val_loss: 0.3947 - val_accuracy: 0.9118\n",
            "Epoch 314/600\n",
            "78/78 [==============================] - 0s 275us/sample - loss: 0.3417 - accuracy: 0.9231 - val_loss: 0.3941 - val_accuracy: 0.9118\n",
            "Epoch 315/600\n",
            "78/78 [==============================] - 0s 273us/sample - loss: 0.3413 - accuracy: 0.9231 - val_loss: 0.3936 - val_accuracy: 0.9118\n",
            "Epoch 316/600\n",
            "78/78 [==============================] - 0s 205us/sample - loss: 0.3408 - accuracy: 0.9231 - val_loss: 0.3931 - val_accuracy: 0.9118\n",
            "Epoch 317/600\n",
            "78/78 [==============================] - 0s 242us/sample - loss: 0.3404 - accuracy: 0.9231 - val_loss: 0.3926 - val_accuracy: 0.9118\n",
            "Epoch 318/600\n",
            "78/78 [==============================] - 0s 250us/sample - loss: 0.3400 - accuracy: 0.9231 - val_loss: 0.3921 - val_accuracy: 0.9118\n",
            "Epoch 319/600\n",
            "78/78 [==============================] - 0s 198us/sample - loss: 0.3395 - accuracy: 0.9231 - val_loss: 0.3916 - val_accuracy: 0.9118\n",
            "Epoch 320/600\n",
            "78/78 [==============================] - 0s 182us/sample - loss: 0.3391 - accuracy: 0.9231 - val_loss: 0.3911 - val_accuracy: 0.9118\n",
            "Epoch 321/600\n",
            "78/78 [==============================] - 0s 179us/sample - loss: 0.3387 - accuracy: 0.9231 - val_loss: 0.3906 - val_accuracy: 0.9118\n",
            "Epoch 322/600\n",
            "78/78 [==============================] - 0s 153us/sample - loss: 0.3382 - accuracy: 0.9231 - val_loss: 0.3901 - val_accuracy: 0.9118\n",
            "Epoch 323/600\n",
            "78/78 [==============================] - 0s 166us/sample - loss: 0.3378 - accuracy: 0.9231 - val_loss: 0.3896 - val_accuracy: 0.9118\n",
            "Epoch 324/600\n",
            "78/78 [==============================] - 0s 186us/sample - loss: 0.3374 - accuracy: 0.9231 - val_loss: 0.3891 - val_accuracy: 0.9118\n",
            "Epoch 325/600\n",
            "78/78 [==============================] - 0s 184us/sample - loss: 0.3370 - accuracy: 0.9359 - val_loss: 0.3886 - val_accuracy: 0.9118\n",
            "Epoch 326/600\n",
            "78/78 [==============================] - 0s 171us/sample - loss: 0.3365 - accuracy: 0.9359 - val_loss: 0.3881 - val_accuracy: 0.9118\n",
            "Epoch 327/600\n",
            "78/78 [==============================] - 0s 164us/sample - loss: 0.3361 - accuracy: 0.9359 - val_loss: 0.3876 - val_accuracy: 0.9118\n",
            "Epoch 328/600\n",
            "78/78 [==============================] - 0s 177us/sample - loss: 0.3357 - accuracy: 0.9359 - val_loss: 0.3871 - val_accuracy: 0.9118\n",
            "Epoch 329/600\n",
            "78/78 [==============================] - 0s 221us/sample - loss: 0.3353 - accuracy: 0.9487 - val_loss: 0.3866 - val_accuracy: 0.9118\n",
            "Epoch 330/600\n",
            "78/78 [==============================] - 0s 232us/sample - loss: 0.3348 - accuracy: 0.9487 - val_loss: 0.3861 - val_accuracy: 0.9118\n",
            "Epoch 331/600\n",
            "78/78 [==============================] - 0s 235us/sample - loss: 0.3344 - accuracy: 0.9487 - val_loss: 0.3856 - val_accuracy: 0.9118\n",
            "Epoch 332/600\n",
            "78/78 [==============================] - 0s 162us/sample - loss: 0.3340 - accuracy: 0.9487 - val_loss: 0.3851 - val_accuracy: 0.9118\n",
            "Epoch 333/600\n",
            "78/78 [==============================] - 0s 229us/sample - loss: 0.3336 - accuracy: 0.9487 - val_loss: 0.3846 - val_accuracy: 0.9118\n",
            "Epoch 334/600\n",
            "78/78 [==============================] - 0s 199us/sample - loss: 0.3331 - accuracy: 0.9487 - val_loss: 0.3841 - val_accuracy: 0.9118\n",
            "Epoch 335/600\n",
            "78/78 [==============================] - 0s 203us/sample - loss: 0.3327 - accuracy: 0.9487 - val_loss: 0.3836 - val_accuracy: 0.9118\n",
            "Epoch 336/600\n",
            "78/78 [==============================] - 0s 135us/sample - loss: 0.3323 - accuracy: 0.9487 - val_loss: 0.3831 - val_accuracy: 0.9118\n",
            "Epoch 337/600\n",
            "78/78 [==============================] - 0s 199us/sample - loss: 0.3319 - accuracy: 0.9487 - val_loss: 0.3826 - val_accuracy: 0.9118\n",
            "Epoch 338/600\n",
            "78/78 [==============================] - 0s 245us/sample - loss: 0.3315 - accuracy: 0.9487 - val_loss: 0.3821 - val_accuracy: 0.9118\n",
            "Epoch 339/600\n",
            "78/78 [==============================] - 0s 205us/sample - loss: 0.3310 - accuracy: 0.9487 - val_loss: 0.3816 - val_accuracy: 0.9118\n",
            "Epoch 340/600\n",
            "78/78 [==============================] - 0s 181us/sample - loss: 0.3306 - accuracy: 0.9487 - val_loss: 0.3811 - val_accuracy: 0.9118\n",
            "Epoch 341/600\n",
            "78/78 [==============================] - 0s 233us/sample - loss: 0.3302 - accuracy: 0.9487 - val_loss: 0.3806 - val_accuracy: 0.9118\n",
            "Epoch 342/600\n",
            "78/78 [==============================] - 0s 181us/sample - loss: 0.3298 - accuracy: 0.9487 - val_loss: 0.3801 - val_accuracy: 0.9118\n",
            "Epoch 343/600\n",
            "78/78 [==============================] - 0s 188us/sample - loss: 0.3294 - accuracy: 0.9487 - val_loss: 0.3797 - val_accuracy: 0.9118\n",
            "Epoch 344/600\n",
            "78/78 [==============================] - 0s 222us/sample - loss: 0.3290 - accuracy: 0.9487 - val_loss: 0.3791 - val_accuracy: 0.9118\n",
            "Epoch 345/600\n",
            "78/78 [==============================] - 0s 199us/sample - loss: 0.3286 - accuracy: 0.9487 - val_loss: 0.3787 - val_accuracy: 0.9118\n",
            "Epoch 346/600\n",
            "78/78 [==============================] - 0s 208us/sample - loss: 0.3281 - accuracy: 0.9487 - val_loss: 0.3782 - val_accuracy: 0.9118\n",
            "Epoch 347/600\n",
            "78/78 [==============================] - 0s 200us/sample - loss: 0.3277 - accuracy: 0.9487 - val_loss: 0.3777 - val_accuracy: 0.9118\n",
            "Epoch 348/600\n",
            "78/78 [==============================] - 0s 207us/sample - loss: 0.3273 - accuracy: 0.9487 - val_loss: 0.3772 - val_accuracy: 0.9118\n",
            "Epoch 349/600\n",
            "78/78 [==============================] - 0s 193us/sample - loss: 0.3269 - accuracy: 0.9487 - val_loss: 0.3767 - val_accuracy: 0.9118\n",
            "Epoch 350/600\n",
            "78/78 [==============================] - 0s 179us/sample - loss: 0.3265 - accuracy: 0.9487 - val_loss: 0.3763 - val_accuracy: 0.9118\n",
            "Epoch 351/600\n",
            "78/78 [==============================] - 0s 227us/sample - loss: 0.3261 - accuracy: 0.9487 - val_loss: 0.3758 - val_accuracy: 0.9118\n",
            "Epoch 352/600\n",
            "78/78 [==============================] - 0s 181us/sample - loss: 0.3257 - accuracy: 0.9487 - val_loss: 0.3753 - val_accuracy: 0.9118\n",
            "Epoch 353/600\n",
            "78/78 [==============================] - 0s 168us/sample - loss: 0.3253 - accuracy: 0.9487 - val_loss: 0.3748 - val_accuracy: 0.9118\n",
            "Epoch 354/600\n",
            "78/78 [==============================] - 0s 201us/sample - loss: 0.3249 - accuracy: 0.9487 - val_loss: 0.3743 - val_accuracy: 0.9118\n",
            "Epoch 355/600\n",
            "78/78 [==============================] - 0s 186us/sample - loss: 0.3245 - accuracy: 0.9487 - val_loss: 0.3739 - val_accuracy: 0.9118\n",
            "Epoch 356/600\n",
            "78/78 [==============================] - 0s 215us/sample - loss: 0.3241 - accuracy: 0.9487 - val_loss: 0.3734 - val_accuracy: 0.9118\n",
            "Epoch 357/600\n",
            "78/78 [==============================] - 0s 225us/sample - loss: 0.3237 - accuracy: 0.9487 - val_loss: 0.3729 - val_accuracy: 0.9118\n",
            "Epoch 358/600\n",
            "78/78 [==============================] - 0s 172us/sample - loss: 0.3233 - accuracy: 0.9487 - val_loss: 0.3724 - val_accuracy: 0.9118\n",
            "Epoch 359/600\n",
            "78/78 [==============================] - 0s 232us/sample - loss: 0.3229 - accuracy: 0.9487 - val_loss: 0.3720 - val_accuracy: 0.9118\n",
            "Epoch 360/600\n",
            "78/78 [==============================] - 0s 227us/sample - loss: 0.3225 - accuracy: 0.9487 - val_loss: 0.3715 - val_accuracy: 0.9118\n",
            "Epoch 361/600\n",
            "78/78 [==============================] - 0s 182us/sample - loss: 0.3221 - accuracy: 0.9487 - val_loss: 0.3710 - val_accuracy: 0.9118\n",
            "Epoch 362/600\n",
            "78/78 [==============================] - 0s 209us/sample - loss: 0.3217 - accuracy: 0.9487 - val_loss: 0.3705 - val_accuracy: 0.9118\n",
            "Epoch 363/600\n",
            "78/78 [==============================] - 0s 200us/sample - loss: 0.3213 - accuracy: 0.9487 - val_loss: 0.3701 - val_accuracy: 0.9118\n",
            "Epoch 364/600\n",
            "78/78 [==============================] - 0s 188us/sample - loss: 0.3209 - accuracy: 0.9487 - val_loss: 0.3696 - val_accuracy: 0.9118\n",
            "Epoch 365/600\n",
            "78/78 [==============================] - 0s 235us/sample - loss: 0.3205 - accuracy: 0.9487 - val_loss: 0.3692 - val_accuracy: 0.9118\n",
            "Epoch 366/600\n",
            "78/78 [==============================] - 0s 238us/sample - loss: 0.3201 - accuracy: 0.9487 - val_loss: 0.3687 - val_accuracy: 0.9118\n",
            "Epoch 367/600\n",
            "78/78 [==============================] - 0s 141us/sample - loss: 0.3197 - accuracy: 0.9487 - val_loss: 0.3682 - val_accuracy: 0.9118\n",
            "Epoch 368/600\n",
            "78/78 [==============================] - 0s 201us/sample - loss: 0.3193 - accuracy: 0.9487 - val_loss: 0.3677 - val_accuracy: 0.9118\n",
            "Epoch 369/600\n",
            "78/78 [==============================] - 0s 235us/sample - loss: 0.3189 - accuracy: 0.9487 - val_loss: 0.3673 - val_accuracy: 0.9118\n",
            "Epoch 370/600\n",
            "78/78 [==============================] - 0s 158us/sample - loss: 0.3185 - accuracy: 0.9487 - val_loss: 0.3668 - val_accuracy: 0.9118\n",
            "Epoch 371/600\n",
            "78/78 [==============================] - 0s 206us/sample - loss: 0.3181 - accuracy: 0.9487 - val_loss: 0.3664 - val_accuracy: 0.9118\n",
            "Epoch 372/600\n",
            "78/78 [==============================] - 0s 212us/sample - loss: 0.3177 - accuracy: 0.9487 - val_loss: 0.3659 - val_accuracy: 0.9118\n",
            "Epoch 373/600\n",
            "78/78 [==============================] - 0s 157us/sample - loss: 0.3173 - accuracy: 0.9487 - val_loss: 0.3655 - val_accuracy: 0.9118\n",
            "Epoch 374/600\n",
            "78/78 [==============================] - 0s 192us/sample - loss: 0.3169 - accuracy: 0.9487 - val_loss: 0.3650 - val_accuracy: 0.9118\n",
            "Epoch 375/600\n",
            "78/78 [==============================] - 0s 147us/sample - loss: 0.3166 - accuracy: 0.9487 - val_loss: 0.3645 - val_accuracy: 0.9118\n",
            "Epoch 376/600\n",
            "78/78 [==============================] - 0s 223us/sample - loss: 0.3162 - accuracy: 0.9487 - val_loss: 0.3641 - val_accuracy: 0.9118\n",
            "Epoch 377/600\n",
            "78/78 [==============================] - 0s 274us/sample - loss: 0.3158 - accuracy: 0.9487 - val_loss: 0.3636 - val_accuracy: 0.9118\n",
            "Epoch 378/600\n",
            "78/78 [==============================] - 0s 265us/sample - loss: 0.3154 - accuracy: 0.9487 - val_loss: 0.3632 - val_accuracy: 0.9118\n",
            "Epoch 379/600\n",
            "78/78 [==============================] - 0s 165us/sample - loss: 0.3150 - accuracy: 0.9487 - val_loss: 0.3627 - val_accuracy: 0.9118\n",
            "Epoch 380/600\n",
            "78/78 [==============================] - 0s 232us/sample - loss: 0.3146 - accuracy: 0.9487 - val_loss: 0.3623 - val_accuracy: 0.9118\n",
            "Epoch 381/600\n",
            "78/78 [==============================] - 0s 208us/sample - loss: 0.3142 - accuracy: 0.9487 - val_loss: 0.3618 - val_accuracy: 0.9118\n",
            "Epoch 382/600\n",
            "78/78 [==============================] - 0s 148us/sample - loss: 0.3139 - accuracy: 0.9487 - val_loss: 0.3614 - val_accuracy: 0.9118\n",
            "Epoch 383/600\n",
            "78/78 [==============================] - 0s 206us/sample - loss: 0.3135 - accuracy: 0.9487 - val_loss: 0.3610 - val_accuracy: 0.9118\n",
            "Epoch 384/600\n",
            "78/78 [==============================] - 0s 204us/sample - loss: 0.3131 - accuracy: 0.9487 - val_loss: 0.3605 - val_accuracy: 0.9118\n",
            "Epoch 385/600\n",
            "78/78 [==============================] - 0s 236us/sample - loss: 0.3127 - accuracy: 0.9487 - val_loss: 0.3600 - val_accuracy: 0.9118\n",
            "Epoch 386/600\n",
            "78/78 [==============================] - 0s 183us/sample - loss: 0.3123 - accuracy: 0.9487 - val_loss: 0.3596 - val_accuracy: 0.9118\n",
            "Epoch 387/600\n",
            "78/78 [==============================] - 0s 142us/sample - loss: 0.3120 - accuracy: 0.9487 - val_loss: 0.3591 - val_accuracy: 0.9118\n",
            "Epoch 388/600\n",
            "78/78 [==============================] - 0s 171us/sample - loss: 0.3116 - accuracy: 0.9487 - val_loss: 0.3587 - val_accuracy: 0.9118\n",
            "Epoch 389/600\n",
            "78/78 [==============================] - 0s 236us/sample - loss: 0.3112 - accuracy: 0.9487 - val_loss: 0.3582 - val_accuracy: 0.9118\n",
            "Epoch 390/600\n",
            "78/78 [==============================] - 0s 311us/sample - loss: 0.3108 - accuracy: 0.9487 - val_loss: 0.3578 - val_accuracy: 0.9118\n",
            "Epoch 391/600\n",
            "78/78 [==============================] - 0s 199us/sample - loss: 0.3104 - accuracy: 0.9487 - val_loss: 0.3573 - val_accuracy: 0.9118\n",
            "Epoch 392/600\n",
            "78/78 [==============================] - 0s 219us/sample - loss: 0.3101 - accuracy: 0.9487 - val_loss: 0.3569 - val_accuracy: 0.9118\n",
            "Epoch 393/600\n",
            "78/78 [==============================] - 0s 213us/sample - loss: 0.3097 - accuracy: 0.9487 - val_loss: 0.3565 - val_accuracy: 0.9118\n",
            "Epoch 394/600\n",
            "78/78 [==============================] - 0s 177us/sample - loss: 0.3093 - accuracy: 0.9487 - val_loss: 0.3560 - val_accuracy: 0.9118\n",
            "Epoch 395/600\n",
            "78/78 [==============================] - 0s 172us/sample - loss: 0.3089 - accuracy: 0.9615 - val_loss: 0.3556 - val_accuracy: 0.9118\n",
            "Epoch 396/600\n",
            "78/78 [==============================] - 0s 226us/sample - loss: 0.3086 - accuracy: 0.9615 - val_loss: 0.3551 - val_accuracy: 0.9118\n",
            "Epoch 397/600\n",
            "78/78 [==============================] - 0s 234us/sample - loss: 0.3082 - accuracy: 0.9615 - val_loss: 0.3547 - val_accuracy: 0.9118\n",
            "Epoch 398/600\n",
            "78/78 [==============================] - 0s 150us/sample - loss: 0.3078 - accuracy: 0.9615 - val_loss: 0.3543 - val_accuracy: 0.9118\n",
            "Epoch 399/600\n",
            "78/78 [==============================] - 0s 225us/sample - loss: 0.3074 - accuracy: 0.9615 - val_loss: 0.3538 - val_accuracy: 0.9118\n",
            "Epoch 400/600\n",
            "78/78 [==============================] - 0s 209us/sample - loss: 0.3071 - accuracy: 0.9615 - val_loss: 0.3534 - val_accuracy: 0.9118\n",
            "Epoch 401/600\n",
            "78/78 [==============================] - 0s 198us/sample - loss: 0.3067 - accuracy: 0.9615 - val_loss: 0.3530 - val_accuracy: 0.9118\n",
            "Epoch 402/600\n",
            "78/78 [==============================] - 0s 220us/sample - loss: 0.3063 - accuracy: 0.9615 - val_loss: 0.3525 - val_accuracy: 0.9118\n",
            "Epoch 403/600\n",
            "78/78 [==============================] - 0s 221us/sample - loss: 0.3060 - accuracy: 0.9615 - val_loss: 0.3521 - val_accuracy: 0.9118\n",
            "Epoch 404/600\n",
            "78/78 [==============================] - 0s 223us/sample - loss: 0.3056 - accuracy: 0.9615 - val_loss: 0.3516 - val_accuracy: 0.9118\n",
            "Epoch 405/600\n",
            "78/78 [==============================] - 0s 221us/sample - loss: 0.3052 - accuracy: 0.9615 - val_loss: 0.3512 - val_accuracy: 0.9118\n",
            "Epoch 406/600\n",
            "78/78 [==============================] - 0s 191us/sample - loss: 0.3049 - accuracy: 0.9615 - val_loss: 0.3508 - val_accuracy: 0.9118\n",
            "Epoch 407/600\n",
            "78/78 [==============================] - 0s 193us/sample - loss: 0.3045 - accuracy: 0.9615 - val_loss: 0.3503 - val_accuracy: 0.9118\n",
            "Epoch 408/600\n",
            "78/78 [==============================] - 0s 241us/sample - loss: 0.3041 - accuracy: 0.9615 - val_loss: 0.3499 - val_accuracy: 0.9118\n",
            "Epoch 409/600\n",
            "78/78 [==============================] - 0s 155us/sample - loss: 0.3038 - accuracy: 0.9615 - val_loss: 0.3494 - val_accuracy: 0.9118\n",
            "Epoch 410/600\n",
            "78/78 [==============================] - 0s 216us/sample - loss: 0.3034 - accuracy: 0.9615 - val_loss: 0.3490 - val_accuracy: 0.9118\n",
            "Epoch 411/600\n",
            "78/78 [==============================] - 0s 222us/sample - loss: 0.3030 - accuracy: 0.9615 - val_loss: 0.3486 - val_accuracy: 0.9118\n",
            "Epoch 412/600\n",
            "78/78 [==============================] - 0s 192us/sample - loss: 0.3027 - accuracy: 0.9615 - val_loss: 0.3482 - val_accuracy: 0.9118\n",
            "Epoch 413/600\n",
            "78/78 [==============================] - 0s 149us/sample - loss: 0.3023 - accuracy: 0.9615 - val_loss: 0.3478 - val_accuracy: 0.9118\n",
            "Epoch 414/600\n",
            "78/78 [==============================] - 0s 191us/sample - loss: 0.3020 - accuracy: 0.9615 - val_loss: 0.3473 - val_accuracy: 0.9118\n",
            "Epoch 415/600\n",
            "78/78 [==============================] - 0s 212us/sample - loss: 0.3016 - accuracy: 0.9615 - val_loss: 0.3469 - val_accuracy: 0.9118\n",
            "Epoch 416/600\n",
            "78/78 [==============================] - 0s 182us/sample - loss: 0.3012 - accuracy: 0.9615 - val_loss: 0.3465 - val_accuracy: 0.9118\n",
            "Epoch 417/600\n",
            "78/78 [==============================] - 0s 185us/sample - loss: 0.3009 - accuracy: 0.9615 - val_loss: 0.3460 - val_accuracy: 0.9118\n",
            "Epoch 418/600\n",
            "78/78 [==============================] - 0s 160us/sample - loss: 0.3005 - accuracy: 0.9615 - val_loss: 0.3456 - val_accuracy: 0.9118\n",
            "Epoch 419/600\n",
            "78/78 [==============================] - 0s 198us/sample - loss: 0.3002 - accuracy: 0.9615 - val_loss: 0.3452 - val_accuracy: 0.9118\n",
            "Epoch 420/600\n",
            "78/78 [==============================] - 0s 196us/sample - loss: 0.2998 - accuracy: 0.9615 - val_loss: 0.3448 - val_accuracy: 0.9118\n",
            "Epoch 421/600\n",
            "78/78 [==============================] - 0s 174us/sample - loss: 0.2994 - accuracy: 0.9615 - val_loss: 0.3443 - val_accuracy: 0.9118\n",
            "Epoch 422/600\n",
            "78/78 [==============================] - 0s 228us/sample - loss: 0.2991 - accuracy: 0.9615 - val_loss: 0.3439 - val_accuracy: 0.9118\n",
            "Epoch 423/600\n",
            "78/78 [==============================] - 0s 237us/sample - loss: 0.2987 - accuracy: 0.9615 - val_loss: 0.3435 - val_accuracy: 0.9118\n",
            "Epoch 424/600\n",
            "78/78 [==============================] - 0s 223us/sample - loss: 0.2984 - accuracy: 0.9615 - val_loss: 0.3431 - val_accuracy: 0.9118\n",
            "Epoch 425/600\n",
            "78/78 [==============================] - 0s 227us/sample - loss: 0.2980 - accuracy: 0.9615 - val_loss: 0.3427 - val_accuracy: 0.9118\n",
            "Epoch 426/600\n",
            "78/78 [==============================] - 0s 190us/sample - loss: 0.2977 - accuracy: 0.9615 - val_loss: 0.3422 - val_accuracy: 0.9118\n",
            "Epoch 427/600\n",
            "78/78 [==============================] - 0s 235us/sample - loss: 0.2973 - accuracy: 0.9615 - val_loss: 0.3418 - val_accuracy: 0.9118\n",
            "Epoch 428/600\n",
            "78/78 [==============================] - 0s 200us/sample - loss: 0.2970 - accuracy: 0.9615 - val_loss: 0.3414 - val_accuracy: 0.9118\n",
            "Epoch 429/600\n",
            "78/78 [==============================] - 0s 169us/sample - loss: 0.2966 - accuracy: 0.9615 - val_loss: 0.3410 - val_accuracy: 0.9118\n",
            "Epoch 430/600\n",
            "78/78 [==============================] - 0s 171us/sample - loss: 0.2963 - accuracy: 0.9615 - val_loss: 0.3405 - val_accuracy: 0.9118\n",
            "Epoch 431/600\n",
            "78/78 [==============================] - 0s 173us/sample - loss: 0.2959 - accuracy: 0.9615 - val_loss: 0.3401 - val_accuracy: 0.9118\n",
            "Epoch 432/600\n",
            "78/78 [==============================] - 0s 318us/sample - loss: 0.2956 - accuracy: 0.9615 - val_loss: 0.3397 - val_accuracy: 0.9118\n",
            "Epoch 433/600\n",
            "78/78 [==============================] - 0s 258us/sample - loss: 0.2952 - accuracy: 0.9615 - val_loss: 0.3393 - val_accuracy: 0.9118\n",
            "Epoch 434/600\n",
            "78/78 [==============================] - 0s 300us/sample - loss: 0.2949 - accuracy: 0.9615 - val_loss: 0.3389 - val_accuracy: 0.9118\n",
            "Epoch 435/600\n",
            "78/78 [==============================] - 0s 263us/sample - loss: 0.2945 - accuracy: 0.9615 - val_loss: 0.3385 - val_accuracy: 0.9118\n",
            "Epoch 436/600\n",
            "78/78 [==============================] - 0s 243us/sample - loss: 0.2942 - accuracy: 0.9615 - val_loss: 0.3381 - val_accuracy: 0.9118\n",
            "Epoch 437/600\n",
            "78/78 [==============================] - 0s 217us/sample - loss: 0.2938 - accuracy: 0.9615 - val_loss: 0.3377 - val_accuracy: 0.9118\n",
            "Epoch 438/600\n",
            "78/78 [==============================] - 0s 164us/sample - loss: 0.2935 - accuracy: 0.9615 - val_loss: 0.3373 - val_accuracy: 0.9118\n",
            "Epoch 439/600\n",
            "78/78 [==============================] - 0s 200us/sample - loss: 0.2931 - accuracy: 0.9615 - val_loss: 0.3369 - val_accuracy: 0.9118\n",
            "Epoch 440/600\n",
            "78/78 [==============================] - 0s 216us/sample - loss: 0.2928 - accuracy: 0.9615 - val_loss: 0.3365 - val_accuracy: 0.9118\n",
            "Epoch 441/600\n",
            "78/78 [==============================] - 0s 263us/sample - loss: 0.2924 - accuracy: 0.9615 - val_loss: 0.3361 - val_accuracy: 0.9118\n",
            "Epoch 442/600\n",
            "78/78 [==============================] - 0s 159us/sample - loss: 0.2921 - accuracy: 0.9615 - val_loss: 0.3356 - val_accuracy: 0.9118\n",
            "Epoch 443/600\n",
            "78/78 [==============================] - 0s 181us/sample - loss: 0.2917 - accuracy: 0.9615 - val_loss: 0.3352 - val_accuracy: 0.9118\n",
            "Epoch 444/600\n",
            "78/78 [==============================] - 0s 225us/sample - loss: 0.2914 - accuracy: 0.9615 - val_loss: 0.3349 - val_accuracy: 0.9118\n",
            "Epoch 445/600\n",
            "78/78 [==============================] - 0s 178us/sample - loss: 0.2911 - accuracy: 0.9615 - val_loss: 0.3344 - val_accuracy: 0.9118\n",
            "Epoch 446/600\n",
            "78/78 [==============================] - 0s 221us/sample - loss: 0.2907 - accuracy: 0.9615 - val_loss: 0.3340 - val_accuracy: 0.9118\n",
            "Epoch 447/600\n",
            "78/78 [==============================] - 0s 188us/sample - loss: 0.2904 - accuracy: 0.9615 - val_loss: 0.3336 - val_accuracy: 0.9118\n",
            "Epoch 448/600\n",
            "78/78 [==============================] - 0s 152us/sample - loss: 0.2900 - accuracy: 0.9615 - val_loss: 0.3332 - val_accuracy: 0.9118\n",
            "Epoch 449/600\n",
            "78/78 [==============================] - 0s 190us/sample - loss: 0.2897 - accuracy: 0.9615 - val_loss: 0.3329 - val_accuracy: 0.9118\n",
            "Epoch 450/600\n",
            "78/78 [==============================] - 0s 208us/sample - loss: 0.2894 - accuracy: 0.9615 - val_loss: 0.3324 - val_accuracy: 0.9118\n",
            "Epoch 451/600\n",
            "78/78 [==============================] - 0s 161us/sample - loss: 0.2890 - accuracy: 0.9615 - val_loss: 0.3320 - val_accuracy: 0.9118\n",
            "Epoch 452/600\n",
            "78/78 [==============================] - 0s 207us/sample - loss: 0.2887 - accuracy: 0.9615 - val_loss: 0.3316 - val_accuracy: 0.9118\n",
            "Epoch 453/600\n",
            "78/78 [==============================] - 0s 228us/sample - loss: 0.2883 - accuracy: 0.9615 - val_loss: 0.3312 - val_accuracy: 0.9118\n",
            "Epoch 454/600\n",
            "78/78 [==============================] - 0s 188us/sample - loss: 0.2880 - accuracy: 0.9615 - val_loss: 0.3308 - val_accuracy: 0.9118\n",
            "Epoch 455/600\n",
            "78/78 [==============================] - 0s 170us/sample - loss: 0.2877 - accuracy: 0.9615 - val_loss: 0.3304 - val_accuracy: 0.9118\n",
            "Epoch 456/600\n",
            "78/78 [==============================] - 0s 212us/sample - loss: 0.2873 - accuracy: 0.9615 - val_loss: 0.3301 - val_accuracy: 0.9118\n",
            "Epoch 457/600\n",
            "78/78 [==============================] - 0s 230us/sample - loss: 0.2870 - accuracy: 0.9615 - val_loss: 0.3297 - val_accuracy: 0.9118\n",
            "Epoch 458/600\n",
            "78/78 [==============================] - 0s 204us/sample - loss: 0.2867 - accuracy: 0.9615 - val_loss: 0.3293 - val_accuracy: 0.9118\n",
            "Epoch 459/600\n",
            "78/78 [==============================] - 0s 188us/sample - loss: 0.2863 - accuracy: 0.9615 - val_loss: 0.3289 - val_accuracy: 0.9118\n",
            "Epoch 460/600\n",
            "78/78 [==============================] - 0s 194us/sample - loss: 0.2860 - accuracy: 0.9615 - val_loss: 0.3285 - val_accuracy: 0.9118\n",
            "Epoch 461/600\n",
            "78/78 [==============================] - 0s 212us/sample - loss: 0.2857 - accuracy: 0.9615 - val_loss: 0.3281 - val_accuracy: 0.9118\n",
            "Epoch 462/600\n",
            "78/78 [==============================] - 0s 225us/sample - loss: 0.2853 - accuracy: 0.9615 - val_loss: 0.3277 - val_accuracy: 0.9118\n",
            "Epoch 463/600\n",
            "78/78 [==============================] - 0s 210us/sample - loss: 0.2850 - accuracy: 0.9615 - val_loss: 0.3273 - val_accuracy: 0.9118\n",
            "Epoch 464/600\n",
            "78/78 [==============================] - 0s 182us/sample - loss: 0.2847 - accuracy: 0.9615 - val_loss: 0.3269 - val_accuracy: 0.9118\n",
            "Epoch 465/600\n",
            "78/78 [==============================] - 0s 193us/sample - loss: 0.2843 - accuracy: 0.9615 - val_loss: 0.3266 - val_accuracy: 0.9118\n",
            "Epoch 466/600\n",
            "78/78 [==============================] - 0s 261us/sample - loss: 0.2840 - accuracy: 0.9615 - val_loss: 0.3261 - val_accuracy: 0.9118\n",
            "Epoch 467/600\n",
            "78/78 [==============================] - 0s 198us/sample - loss: 0.2837 - accuracy: 0.9615 - val_loss: 0.3257 - val_accuracy: 0.9118\n",
            "Epoch 468/600\n",
            "78/78 [==============================] - 0s 200us/sample - loss: 0.2833 - accuracy: 0.9615 - val_loss: 0.3254 - val_accuracy: 0.9118\n",
            "Epoch 469/600\n",
            "78/78 [==============================] - 0s 213us/sample - loss: 0.2830 - accuracy: 0.9615 - val_loss: 0.3250 - val_accuracy: 0.9118\n",
            "Epoch 470/600\n",
            "78/78 [==============================] - 0s 196us/sample - loss: 0.2827 - accuracy: 0.9615 - val_loss: 0.3246 - val_accuracy: 0.9118\n",
            "Epoch 471/600\n",
            "78/78 [==============================] - 0s 186us/sample - loss: 0.2824 - accuracy: 0.9615 - val_loss: 0.3242 - val_accuracy: 0.9118\n",
            "Epoch 472/600\n",
            "78/78 [==============================] - 0s 213us/sample - loss: 0.2820 - accuracy: 0.9615 - val_loss: 0.3239 - val_accuracy: 0.9118\n",
            "Epoch 473/600\n",
            "78/78 [==============================] - 0s 220us/sample - loss: 0.2817 - accuracy: 0.9615 - val_loss: 0.3235 - val_accuracy: 0.9118\n",
            "Epoch 474/600\n",
            "78/78 [==============================] - 0s 209us/sample - loss: 0.2814 - accuracy: 0.9615 - val_loss: 0.3231 - val_accuracy: 0.9118\n",
            "Epoch 475/600\n",
            "78/78 [==============================] - 0s 179us/sample - loss: 0.2810 - accuracy: 0.9615 - val_loss: 0.3227 - val_accuracy: 0.9118\n",
            "Epoch 476/600\n",
            "78/78 [==============================] - 0s 196us/sample - loss: 0.2807 - accuracy: 0.9615 - val_loss: 0.3223 - val_accuracy: 0.9118\n",
            "Epoch 477/600\n",
            "78/78 [==============================] - 0s 235us/sample - loss: 0.2804 - accuracy: 0.9615 - val_loss: 0.3219 - val_accuracy: 0.9118\n",
            "Epoch 478/600\n",
            "78/78 [==============================] - 0s 227us/sample - loss: 0.2801 - accuracy: 0.9615 - val_loss: 0.3215 - val_accuracy: 0.9118\n",
            "Epoch 479/600\n",
            "78/78 [==============================] - 0s 183us/sample - loss: 0.2797 - accuracy: 0.9615 - val_loss: 0.3212 - val_accuracy: 0.9118\n",
            "Epoch 480/600\n",
            "78/78 [==============================] - 0s 217us/sample - loss: 0.2794 - accuracy: 0.9615 - val_loss: 0.3208 - val_accuracy: 0.9118\n",
            "Epoch 481/600\n",
            "78/78 [==============================] - 0s 194us/sample - loss: 0.2791 - accuracy: 0.9615 - val_loss: 0.3204 - val_accuracy: 0.9118\n",
            "Epoch 482/600\n",
            "78/78 [==============================] - 0s 204us/sample - loss: 0.2788 - accuracy: 0.9615 - val_loss: 0.3200 - val_accuracy: 0.9118\n",
            "Epoch 483/600\n",
            "78/78 [==============================] - 0s 185us/sample - loss: 0.2785 - accuracy: 0.9615 - val_loss: 0.3197 - val_accuracy: 0.9118\n",
            "Epoch 484/600\n",
            "78/78 [==============================] - 0s 301us/sample - loss: 0.2781 - accuracy: 0.9615 - val_loss: 0.3193 - val_accuracy: 0.9118\n",
            "Epoch 485/600\n",
            "78/78 [==============================] - 0s 225us/sample - loss: 0.2778 - accuracy: 0.9615 - val_loss: 0.3189 - val_accuracy: 0.9118\n",
            "Epoch 486/600\n",
            "78/78 [==============================] - 0s 202us/sample - loss: 0.2775 - accuracy: 0.9615 - val_loss: 0.3186 - val_accuracy: 0.9118\n",
            "Epoch 487/600\n",
            "78/78 [==============================] - 0s 230us/sample - loss: 0.2772 - accuracy: 0.9615 - val_loss: 0.3182 - val_accuracy: 0.9118\n",
            "Epoch 488/600\n",
            "78/78 [==============================] - 0s 247us/sample - loss: 0.2769 - accuracy: 0.9615 - val_loss: 0.3178 - val_accuracy: 0.9118\n",
            "Epoch 489/600\n",
            "78/78 [==============================] - 0s 194us/sample - loss: 0.2765 - accuracy: 0.9615 - val_loss: 0.3174 - val_accuracy: 0.9118\n",
            "Epoch 490/600\n",
            "78/78 [==============================] - 0s 204us/sample - loss: 0.2762 - accuracy: 0.9615 - val_loss: 0.3171 - val_accuracy: 0.9118\n",
            "Epoch 491/600\n",
            "78/78 [==============================] - 0s 141us/sample - loss: 0.2759 - accuracy: 0.9615 - val_loss: 0.3167 - val_accuracy: 0.9118\n",
            "Epoch 492/600\n",
            "78/78 [==============================] - 0s 183us/sample - loss: 0.2756 - accuracy: 0.9615 - val_loss: 0.3163 - val_accuracy: 0.9118\n",
            "Epoch 493/600\n",
            "78/78 [==============================] - 0s 283us/sample - loss: 0.2753 - accuracy: 0.9615 - val_loss: 0.3160 - val_accuracy: 0.9118\n",
            "Epoch 494/600\n",
            "78/78 [==============================] - 0s 213us/sample - loss: 0.2750 - accuracy: 0.9615 - val_loss: 0.3156 - val_accuracy: 0.9118\n",
            "Epoch 495/600\n",
            "78/78 [==============================] - 0s 273us/sample - loss: 0.2746 - accuracy: 0.9615 - val_loss: 0.3152 - val_accuracy: 0.9118\n",
            "Epoch 496/600\n",
            "78/78 [==============================] - 0s 211us/sample - loss: 0.2743 - accuracy: 0.9615 - val_loss: 0.3148 - val_accuracy: 0.9118\n",
            "Epoch 497/600\n",
            "78/78 [==============================] - 0s 162us/sample - loss: 0.2740 - accuracy: 0.9615 - val_loss: 0.3145 - val_accuracy: 0.9118\n",
            "Epoch 498/600\n",
            "78/78 [==============================] - 0s 177us/sample - loss: 0.2737 - accuracy: 0.9615 - val_loss: 0.3141 - val_accuracy: 0.9118\n",
            "Epoch 499/600\n",
            "78/78 [==============================] - 0s 196us/sample - loss: 0.2734 - accuracy: 0.9615 - val_loss: 0.3137 - val_accuracy: 0.9118\n",
            "Epoch 500/600\n",
            "78/78 [==============================] - 0s 215us/sample - loss: 0.2731 - accuracy: 0.9615 - val_loss: 0.3134 - val_accuracy: 0.9118\n",
            "Epoch 501/600\n",
            "78/78 [==============================] - 0s 162us/sample - loss: 0.2728 - accuracy: 0.9615 - val_loss: 0.3130 - val_accuracy: 0.9118\n",
            "Epoch 502/600\n",
            "78/78 [==============================] - 0s 188us/sample - loss: 0.2725 - accuracy: 0.9615 - val_loss: 0.3126 - val_accuracy: 0.9118\n",
            "Epoch 503/600\n",
            "78/78 [==============================] - 0s 232us/sample - loss: 0.2722 - accuracy: 0.9615 - val_loss: 0.3123 - val_accuracy: 0.9118\n",
            "Epoch 504/600\n",
            "78/78 [==============================] - 0s 183us/sample - loss: 0.2718 - accuracy: 0.9615 - val_loss: 0.3119 - val_accuracy: 0.9118\n",
            "Epoch 505/600\n",
            "78/78 [==============================] - 0s 175us/sample - loss: 0.2715 - accuracy: 0.9615 - val_loss: 0.3116 - val_accuracy: 0.9118\n",
            "Epoch 506/600\n",
            "78/78 [==============================] - 0s 248us/sample - loss: 0.2712 - accuracy: 0.9615 - val_loss: 0.3112 - val_accuracy: 0.9118\n",
            "Epoch 507/600\n",
            "78/78 [==============================] - 0s 264us/sample - loss: 0.2709 - accuracy: 0.9615 - val_loss: 0.3108 - val_accuracy: 0.9118\n",
            "Epoch 508/600\n",
            "78/78 [==============================] - 0s 188us/sample - loss: 0.2706 - accuracy: 0.9615 - val_loss: 0.3104 - val_accuracy: 0.9118\n",
            "Epoch 509/600\n",
            "78/78 [==============================] - 0s 213us/sample - loss: 0.2703 - accuracy: 0.9615 - val_loss: 0.3101 - val_accuracy: 0.9118\n",
            "Epoch 510/600\n",
            "78/78 [==============================] - 0s 198us/sample - loss: 0.2700 - accuracy: 0.9615 - val_loss: 0.3097 - val_accuracy: 0.9118\n",
            "Epoch 511/600\n",
            "78/78 [==============================] - 0s 174us/sample - loss: 0.2697 - accuracy: 0.9615 - val_loss: 0.3094 - val_accuracy: 0.9118\n",
            "Epoch 512/600\n",
            "78/78 [==============================] - 0s 184us/sample - loss: 0.2694 - accuracy: 0.9615 - val_loss: 0.3090 - val_accuracy: 0.9118\n",
            "Epoch 513/600\n",
            "78/78 [==============================] - 0s 184us/sample - loss: 0.2691 - accuracy: 0.9615 - val_loss: 0.3086 - val_accuracy: 0.9118\n",
            "Epoch 514/600\n",
            "78/78 [==============================] - 0s 170us/sample - loss: 0.2688 - accuracy: 0.9615 - val_loss: 0.3083 - val_accuracy: 0.9118\n",
            "Epoch 515/600\n",
            "78/78 [==============================] - 0s 202us/sample - loss: 0.2685 - accuracy: 0.9615 - val_loss: 0.3079 - val_accuracy: 0.9118\n",
            "Epoch 516/600\n",
            "78/78 [==============================] - 0s 147us/sample - loss: 0.2682 - accuracy: 0.9615 - val_loss: 0.3076 - val_accuracy: 0.9118\n",
            "Epoch 517/600\n",
            "78/78 [==============================] - 0s 230us/sample - loss: 0.2679 - accuracy: 0.9615 - val_loss: 0.3072 - val_accuracy: 0.9118\n",
            "Epoch 518/600\n",
            "78/78 [==============================] - 0s 169us/sample - loss: 0.2676 - accuracy: 0.9615 - val_loss: 0.3068 - val_accuracy: 0.9118\n",
            "Epoch 519/600\n",
            "78/78 [==============================] - 0s 212us/sample - loss: 0.2673 - accuracy: 0.9615 - val_loss: 0.3065 - val_accuracy: 0.9118\n",
            "Epoch 520/600\n",
            "78/78 [==============================] - 0s 251us/sample - loss: 0.2670 - accuracy: 0.9615 - val_loss: 0.3061 - val_accuracy: 0.9118\n",
            "Epoch 521/600\n",
            "78/78 [==============================] - 0s 187us/sample - loss: 0.2667 - accuracy: 0.9615 - val_loss: 0.3058 - val_accuracy: 0.9118\n",
            "Epoch 522/600\n",
            "78/78 [==============================] - 0s 144us/sample - loss: 0.2664 - accuracy: 0.9615 - val_loss: 0.3054 - val_accuracy: 0.9118\n",
            "Epoch 523/600\n",
            "78/78 [==============================] - 0s 222us/sample - loss: 0.2661 - accuracy: 0.9615 - val_loss: 0.3051 - val_accuracy: 0.9118\n",
            "Epoch 524/600\n",
            "78/78 [==============================] - 0s 233us/sample - loss: 0.2658 - accuracy: 0.9615 - val_loss: 0.3047 - val_accuracy: 0.9118\n",
            "Epoch 525/600\n",
            "78/78 [==============================] - 0s 206us/sample - loss: 0.2655 - accuracy: 0.9615 - val_loss: 0.3044 - val_accuracy: 0.9118\n",
            "Epoch 526/600\n",
            "78/78 [==============================] - 0s 198us/sample - loss: 0.2652 - accuracy: 0.9615 - val_loss: 0.3040 - val_accuracy: 0.9118\n",
            "Epoch 527/600\n",
            "78/78 [==============================] - 0s 194us/sample - loss: 0.2649 - accuracy: 0.9615 - val_loss: 0.3037 - val_accuracy: 0.9118\n",
            "Epoch 528/600\n",
            "78/78 [==============================] - 0s 157us/sample - loss: 0.2646 - accuracy: 0.9615 - val_loss: 0.3033 - val_accuracy: 0.9118\n",
            "Epoch 529/600\n",
            "78/78 [==============================] - 0s 199us/sample - loss: 0.2643 - accuracy: 0.9615 - val_loss: 0.3030 - val_accuracy: 0.9118\n",
            "Epoch 530/600\n",
            "78/78 [==============================] - 0s 213us/sample - loss: 0.2640 - accuracy: 0.9615 - val_loss: 0.3026 - val_accuracy: 0.9118\n",
            "Epoch 531/600\n",
            "78/78 [==============================] - 0s 222us/sample - loss: 0.2637 - accuracy: 0.9615 - val_loss: 0.3023 - val_accuracy: 0.9118\n",
            "Epoch 532/600\n",
            "78/78 [==============================] - 0s 226us/sample - loss: 0.2634 - accuracy: 0.9615 - val_loss: 0.3019 - val_accuracy: 0.9118\n",
            "Epoch 533/600\n",
            "78/78 [==============================] - 0s 176us/sample - loss: 0.2631 - accuracy: 0.9615 - val_loss: 0.3016 - val_accuracy: 0.9118\n",
            "Epoch 534/600\n",
            "78/78 [==============================] - 0s 174us/sample - loss: 0.2628 - accuracy: 0.9615 - val_loss: 0.3013 - val_accuracy: 0.9118\n",
            "Epoch 535/600\n",
            "78/78 [==============================] - 0s 151us/sample - loss: 0.2625 - accuracy: 0.9615 - val_loss: 0.3009 - val_accuracy: 0.9118\n",
            "Epoch 536/600\n",
            "78/78 [==============================] - 0s 191us/sample - loss: 0.2622 - accuracy: 0.9615 - val_loss: 0.3005 - val_accuracy: 0.9118\n",
            "Epoch 537/600\n",
            "78/78 [==============================] - 0s 201us/sample - loss: 0.2619 - accuracy: 0.9615 - val_loss: 0.3002 - val_accuracy: 0.9118\n",
            "Epoch 538/600\n",
            "78/78 [==============================] - 0s 197us/sample - loss: 0.2616 - accuracy: 0.9615 - val_loss: 0.2998 - val_accuracy: 0.9118\n",
            "Epoch 539/600\n",
            "78/78 [==============================] - 0s 241us/sample - loss: 0.2613 - accuracy: 0.9615 - val_loss: 0.2995 - val_accuracy: 0.9118\n",
            "Epoch 540/600\n",
            "78/78 [==============================] - 0s 191us/sample - loss: 0.2611 - accuracy: 0.9615 - val_loss: 0.2992 - val_accuracy: 0.9118\n",
            "Epoch 541/600\n",
            "78/78 [==============================] - 0s 184us/sample - loss: 0.2608 - accuracy: 0.9615 - val_loss: 0.2988 - val_accuracy: 0.9118\n",
            "Epoch 542/600\n",
            "78/78 [==============================] - 0s 243us/sample - loss: 0.2605 - accuracy: 0.9615 - val_loss: 0.2985 - val_accuracy: 0.9118\n",
            "Epoch 543/600\n",
            "78/78 [==============================] - 0s 140us/sample - loss: 0.2602 - accuracy: 0.9615 - val_loss: 0.2982 - val_accuracy: 0.9118\n",
            "Epoch 544/600\n",
            "78/78 [==============================] - 0s 212us/sample - loss: 0.2599 - accuracy: 0.9615 - val_loss: 0.2978 - val_accuracy: 0.9118\n",
            "Epoch 545/600\n",
            "78/78 [==============================] - 0s 179us/sample - loss: 0.2596 - accuracy: 0.9615 - val_loss: 0.2975 - val_accuracy: 0.9118\n",
            "Epoch 546/600\n",
            "78/78 [==============================] - 0s 244us/sample - loss: 0.2593 - accuracy: 0.9615 - val_loss: 0.2971 - val_accuracy: 0.9118\n",
            "Epoch 547/600\n",
            "78/78 [==============================] - 0s 215us/sample - loss: 0.2590 - accuracy: 0.9615 - val_loss: 0.2968 - val_accuracy: 0.9118\n",
            "Epoch 548/600\n",
            "78/78 [==============================] - 0s 212us/sample - loss: 0.2587 - accuracy: 0.9615 - val_loss: 0.2965 - val_accuracy: 0.9118\n",
            "Epoch 549/600\n",
            "78/78 [==============================] - 0s 192us/sample - loss: 0.2585 - accuracy: 0.9615 - val_loss: 0.2961 - val_accuracy: 0.9118\n",
            "Epoch 550/600\n",
            "78/78 [==============================] - 0s 171us/sample - loss: 0.2582 - accuracy: 0.9615 - val_loss: 0.2958 - val_accuracy: 0.9118\n",
            "Epoch 551/600\n",
            "78/78 [==============================] - 0s 156us/sample - loss: 0.2579 - accuracy: 0.9615 - val_loss: 0.2954 - val_accuracy: 0.9118\n",
            "Epoch 552/600\n",
            "78/78 [==============================] - 0s 203us/sample - loss: 0.2576 - accuracy: 0.9615 - val_loss: 0.2951 - val_accuracy: 0.9118\n",
            "Epoch 553/600\n",
            "78/78 [==============================] - 0s 218us/sample - loss: 0.2573 - accuracy: 0.9615 - val_loss: 0.2948 - val_accuracy: 0.9118\n",
            "Epoch 554/600\n",
            "78/78 [==============================] - 0s 184us/sample - loss: 0.2570 - accuracy: 0.9615 - val_loss: 0.2944 - val_accuracy: 0.9118\n",
            "Epoch 555/600\n",
            "78/78 [==============================] - 0s 177us/sample - loss: 0.2568 - accuracy: 0.9615 - val_loss: 0.2941 - val_accuracy: 0.9118\n",
            "Epoch 556/600\n",
            "78/78 [==============================] - 0s 219us/sample - loss: 0.2565 - accuracy: 0.9615 - val_loss: 0.2938 - val_accuracy: 0.9118\n",
            "Epoch 557/600\n",
            "78/78 [==============================] - 0s 237us/sample - loss: 0.2562 - accuracy: 0.9615 - val_loss: 0.2935 - val_accuracy: 0.9118\n",
            "Epoch 558/600\n",
            "78/78 [==============================] - 0s 183us/sample - loss: 0.2559 - accuracy: 0.9615 - val_loss: 0.2931 - val_accuracy: 0.9118\n",
            "Epoch 559/600\n",
            "78/78 [==============================] - 0s 177us/sample - loss: 0.2556 - accuracy: 0.9615 - val_loss: 0.2928 - val_accuracy: 0.9118\n",
            "Epoch 560/600\n",
            "78/78 [==============================] - 0s 212us/sample - loss: 0.2553 - accuracy: 0.9615 - val_loss: 0.2924 - val_accuracy: 0.9118\n",
            "Epoch 561/600\n",
            "78/78 [==============================] - 0s 191us/sample - loss: 0.2551 - accuracy: 0.9615 - val_loss: 0.2921 - val_accuracy: 0.9118\n",
            "Epoch 562/600\n",
            "78/78 [==============================] - 0s 173us/sample - loss: 0.2548 - accuracy: 0.9615 - val_loss: 0.2918 - val_accuracy: 0.9412\n",
            "Epoch 563/600\n",
            "78/78 [==============================] - 0s 207us/sample - loss: 0.2545 - accuracy: 0.9615 - val_loss: 0.2915 - val_accuracy: 0.9412\n",
            "Epoch 564/600\n",
            "78/78 [==============================] - 0s 174us/sample - loss: 0.2542 - accuracy: 0.9615 - val_loss: 0.2911 - val_accuracy: 0.9412\n",
            "Epoch 565/600\n",
            "78/78 [==============================] - 0s 196us/sample - loss: 0.2539 - accuracy: 0.9615 - val_loss: 0.2908 - val_accuracy: 0.9412\n",
            "Epoch 566/600\n",
            "78/78 [==============================] - 0s 208us/sample - loss: 0.2537 - accuracy: 0.9615 - val_loss: 0.2905 - val_accuracy: 0.9412\n",
            "Epoch 567/600\n",
            "78/78 [==============================] - 0s 208us/sample - loss: 0.2534 - accuracy: 0.9615 - val_loss: 0.2901 - val_accuracy: 0.9412\n",
            "Epoch 568/600\n",
            "78/78 [==============================] - 0s 206us/sample - loss: 0.2531 - accuracy: 0.9615 - val_loss: 0.2898 - val_accuracy: 0.9412\n",
            "Epoch 569/600\n",
            "78/78 [==============================] - 0s 184us/sample - loss: 0.2528 - accuracy: 0.9615 - val_loss: 0.2895 - val_accuracy: 0.9412\n",
            "Epoch 570/600\n",
            "78/78 [==============================] - 0s 206us/sample - loss: 0.2526 - accuracy: 0.9615 - val_loss: 0.2892 - val_accuracy: 0.9412\n",
            "Epoch 571/600\n",
            "78/78 [==============================] - 0s 261us/sample - loss: 0.2523 - accuracy: 0.9615 - val_loss: 0.2889 - val_accuracy: 0.9412\n",
            "Epoch 572/600\n",
            "78/78 [==============================] - 0s 216us/sample - loss: 0.2520 - accuracy: 0.9615 - val_loss: 0.2885 - val_accuracy: 0.9412\n",
            "Epoch 573/600\n",
            "78/78 [==============================] - 0s 203us/sample - loss: 0.2517 - accuracy: 0.9615 - val_loss: 0.2882 - val_accuracy: 0.9412\n",
            "Epoch 574/600\n",
            "78/78 [==============================] - 0s 162us/sample - loss: 0.2515 - accuracy: 0.9615 - val_loss: 0.2879 - val_accuracy: 0.9412\n",
            "Epoch 575/600\n",
            "78/78 [==============================] - 0s 188us/sample - loss: 0.2512 - accuracy: 0.9615 - val_loss: 0.2875 - val_accuracy: 0.9412\n",
            "Epoch 576/600\n",
            "78/78 [==============================] - 0s 200us/sample - loss: 0.2509 - accuracy: 0.9615 - val_loss: 0.2872 - val_accuracy: 0.9412\n",
            "Epoch 577/600\n",
            "78/78 [==============================] - 0s 261us/sample - loss: 0.2506 - accuracy: 0.9615 - val_loss: 0.2869 - val_accuracy: 0.9412\n",
            "Epoch 578/600\n",
            "78/78 [==============================] - 0s 169us/sample - loss: 0.2504 - accuracy: 0.9615 - val_loss: 0.2866 - val_accuracy: 0.9412\n",
            "Epoch 579/600\n",
            "78/78 [==============================] - 0s 219us/sample - loss: 0.2501 - accuracy: 0.9615 - val_loss: 0.2863 - val_accuracy: 0.9412\n",
            "Epoch 580/600\n",
            "78/78 [==============================] - 0s 197us/sample - loss: 0.2498 - accuracy: 0.9615 - val_loss: 0.2860 - val_accuracy: 0.9412\n",
            "Epoch 581/600\n",
            "78/78 [==============================] - 0s 137us/sample - loss: 0.2496 - accuracy: 0.9615 - val_loss: 0.2856 - val_accuracy: 0.9412\n",
            "Epoch 582/600\n",
            "78/78 [==============================] - 0s 194us/sample - loss: 0.2493 - accuracy: 0.9615 - val_loss: 0.2853 - val_accuracy: 0.9412\n",
            "Epoch 583/600\n",
            "78/78 [==============================] - 0s 197us/sample - loss: 0.2490 - accuracy: 0.9615 - val_loss: 0.2850 - val_accuracy: 0.9412\n",
            "Epoch 584/600\n",
            "78/78 [==============================] - 0s 239us/sample - loss: 0.2488 - accuracy: 0.9615 - val_loss: 0.2847 - val_accuracy: 0.9412\n",
            "Epoch 585/600\n",
            "78/78 [==============================] - 0s 187us/sample - loss: 0.2485 - accuracy: 0.9615 - val_loss: 0.2844 - val_accuracy: 0.9412\n",
            "Epoch 586/600\n",
            "78/78 [==============================] - 0s 278us/sample - loss: 0.2482 - accuracy: 0.9615 - val_loss: 0.2841 - val_accuracy: 0.9412\n",
            "Epoch 587/600\n",
            "78/78 [==============================] - 0s 199us/sample - loss: 0.2479 - accuracy: 0.9615 - val_loss: 0.2838 - val_accuracy: 0.9412\n",
            "Epoch 588/600\n",
            "78/78 [==============================] - 0s 169us/sample - loss: 0.2477 - accuracy: 0.9615 - val_loss: 0.2834 - val_accuracy: 0.9412\n",
            "Epoch 589/600\n",
            "78/78 [==============================] - 0s 231us/sample - loss: 0.2474 - accuracy: 0.9615 - val_loss: 0.2831 - val_accuracy: 0.9412\n",
            "Epoch 590/600\n",
            "78/78 [==============================] - 0s 183us/sample - loss: 0.2471 - accuracy: 0.9615 - val_loss: 0.2828 - val_accuracy: 0.9412\n",
            "Epoch 591/600\n",
            "78/78 [==============================] - 0s 269us/sample - loss: 0.2469 - accuracy: 0.9615 - val_loss: 0.2825 - val_accuracy: 0.9412\n",
            "Epoch 592/600\n",
            "78/78 [==============================] - 0s 150us/sample - loss: 0.2466 - accuracy: 0.9615 - val_loss: 0.2822 - val_accuracy: 0.9412\n",
            "Epoch 593/600\n",
            "78/78 [==============================] - 0s 199us/sample - loss: 0.2464 - accuracy: 0.9615 - val_loss: 0.2819 - val_accuracy: 0.9412\n",
            "Epoch 594/600\n",
            "78/78 [==============================] - 0s 192us/sample - loss: 0.2461 - accuracy: 0.9615 - val_loss: 0.2816 - val_accuracy: 0.9412\n",
            "Epoch 595/600\n",
            "78/78 [==============================] - 0s 219us/sample - loss: 0.2458 - accuracy: 0.9615 - val_loss: 0.2812 - val_accuracy: 0.9412\n",
            "Epoch 596/600\n",
            "78/78 [==============================] - 0s 209us/sample - loss: 0.2456 - accuracy: 0.9615 - val_loss: 0.2810 - val_accuracy: 0.9412\n",
            "Epoch 597/600\n",
            "78/78 [==============================] - 0s 230us/sample - loss: 0.2453 - accuracy: 0.9615 - val_loss: 0.2806 - val_accuracy: 0.9412\n",
            "Epoch 598/600\n",
            "78/78 [==============================] - 0s 196us/sample - loss: 0.2450 - accuracy: 0.9615 - val_loss: 0.2803 - val_accuracy: 0.9412\n",
            "Epoch 599/600\n",
            "78/78 [==============================] - 0s 174us/sample - loss: 0.2448 - accuracy: 0.9615 - val_loss: 0.2800 - val_accuracy: 0.9412\n",
            "Epoch 600/600\n",
            "78/78 [==============================] - 0s 177us/sample - loss: 0.2445 - accuracy: 0.9615 - val_loss: 0.2797 - val_accuracy: 0.9412\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi7LsS7xhPDk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "ab8bf9ee-be8a-43a6-93b2-69f62d592cdb"
      },
      "source": [
        "print(history.history['val_accuracy'])\n",
        "\n",
        "print(history.history['accuracy'])\n",
        "\n",
        "ta = pd.DataFrame(history.history['accuracy'])\n",
        "va = pd.DataFrame(history.history['val_accuracy'])\n",
        "\n",
        "tva = pd.concat([ta,va] , axis=1)\n",
        "\n",
        "tva.boxplot()"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.32352942, 0.32352942, 0.32352942, 0.32352942, 0.32352942, 0.32352942, 0.32352942, 0.32352942, 0.32352942, 0.32352942, 0.32352942, 0.3529412, 0.5588235, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.61764705, 0.64705884, 0.64705884, 0.64705884, 0.64705884, 0.64705884, 0.64705884, 0.64705884, 0.64705884, 0.64705884, 0.64705884, 0.64705884, 0.64705884, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7058824, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7352941, 0.7647059, 0.7647059, 0.7647059, 0.7647059, 0.7647059, 0.7647059, 0.7647059, 0.7647059, 0.7647059, 0.7647059, 0.7647059, 0.7647059, 0.7647059, 0.7647059, 0.7647059, 0.7647059, 0.7647059, 0.7647059, 0.7647059, 0.7941176, 0.7941176, 0.7941176, 0.7941176, 0.8235294, 0.8235294, 0.8235294, 0.8235294, 0.8235294, 0.8235294, 0.8235294, 0.8235294, 0.8235294, 0.8235294, 0.8235294, 0.8235294, 0.8235294, 0.8235294, 0.8235294, 0.8235294, 0.8235294, 0.8235294, 0.8235294, 0.8235294, 0.8235294, 0.8235294, 0.8235294, 0.8235294, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.85294116, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.88235295, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9117647, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765, 0.9411765]\n",
            "[0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.33333334, 0.6282051, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.74358976, 0.75641024, 0.75641024, 0.75641024, 0.7692308, 0.7692308, 0.7692308, 0.7692308, 0.7692308, 0.7692308, 0.7692308, 0.7692308, 0.7692308, 0.7692308, 0.7692308, 0.7692308, 0.7692308, 0.7692308, 0.7692308, 0.7692308, 0.7692308, 0.7692308, 0.7692308, 0.78205127, 0.78205127, 0.78205127, 0.78205127, 0.78205127, 0.78205127, 0.78205127, 0.78205127, 0.78205127, 0.78205127, 0.78205127, 0.78205127, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.7948718, 0.8076923, 0.8076923, 0.8076923, 0.8076923, 0.8076923, 0.8076923, 0.8076923, 0.8076923, 0.8076923, 0.8076923, 0.8076923, 0.8076923, 0.8076923, 0.8076923, 0.8076923, 0.8076923, 0.82051283, 0.82051283, 0.82051283, 0.82051283, 0.82051283, 0.82051283, 0.82051283, 0.8333333, 0.8333333, 0.8333333, 0.8333333, 0.8333333, 0.8333333, 0.8333333, 0.84615386, 0.84615386, 0.84615386, 0.84615386, 0.84615386, 0.84615386, 0.84615386, 0.84615386, 0.85897434, 0.8717949, 0.8717949, 0.8717949, 0.8717949, 0.8717949, 0.8717949, 0.8717949, 0.8717949, 0.8717949, 0.8717949, 0.8717949, 0.8717949, 0.8717949, 0.8717949, 0.8717949, 0.8717949, 0.8717949, 0.8717949, 0.8717949, 0.8717949, 0.8717949, 0.8717949, 0.8717949, 0.88461536, 0.88461536, 0.88461536, 0.88461536, 0.88461536, 0.88461536, 0.88461536, 0.88461536, 0.88461536, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.8974359, 0.9102564, 0.9102564, 0.9102564, 0.9102564, 0.9102564, 0.9102564, 0.9102564, 0.9230769, 0.9230769, 0.9230769, 0.9230769, 0.9230769, 0.9230769, 0.9230769, 0.9230769, 0.9230769, 0.9230769, 0.9230769, 0.9230769, 0.9230769, 0.9230769, 0.9230769, 0.9230769, 0.9230769, 0.9230769, 0.9358974, 0.9358974, 0.9358974, 0.9358974, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.94871795, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fbb0c793e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ny2kikhgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSwbRdfAj6QZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7dd5766a-95ea-42f5-c4c6-012cf9fd7c90"
      },
      "source": [
        "loss, acc = model.evaluate(X_test, ytest, verbose=0)\n",
        "print('Accuracy: %.3f'  % acc)\n",
        "print('Loss: %.3f' % loss)"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.895\n",
            "Loss: 0.311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re9ItAR3yS3J",
        "colab_type": "text"
      },
      "source": [
        "## Question 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liw0IFf9yVqH",
        "colab_type": "text"
      },
      "source": [
        "### Make predictions\n",
        "- Predict labels on one row"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5sBybi6mlLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_predict = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjMIq86kpAo_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "b612e47f-5b79-42db-baeb-f3ccfd77b60d"
      },
      "source": [
        "y_predict"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.93934453e-01, 6.05838886e-03, 7.16224667e-06],\n",
              "       [1.82192355e-01, 7.03058898e-01, 1.14748694e-01],\n",
              "       [4.60018255e-02, 7.37763464e-01, 2.16234684e-01],\n",
              "       [9.85986114e-01, 1.39823565e-02, 3.15308680e-05],\n",
              "       [2.73971562e-03, 3.24967086e-01, 6.72293246e-01],\n",
              "       [2.37251241e-02, 6.07228339e-01, 3.69046539e-01],\n",
              "       [5.58600202e-03, 4.13167030e-01, 5.81246912e-01],\n",
              "       [9.48528767e-01, 5.10953367e-02, 3.75975389e-04],\n",
              "       [9.49362934e-01, 5.01278453e-02, 5.09180652e-04],\n",
              "       [1.64301367e-03, 2.06050843e-01, 7.92306185e-01],\n",
              "       [2.81889271e-02, 5.58076501e-01, 4.13734555e-01],\n",
              "       [9.72416580e-01, 2.74743903e-02, 1.09023946e-04],\n",
              "       [1.33152376e-03, 1.75443843e-01, 8.23224664e-01],\n",
              "       [3.77197862e-02, 6.93757355e-01, 2.68522859e-01],\n",
              "       [1.99444909e-02, 5.43927789e-01, 4.36127633e-01],\n",
              "       [9.62795913e-01, 3.68922167e-02, 3.11835815e-04],\n",
              "       [4.84343246e-02, 6.68601036e-01, 2.82964647e-01],\n",
              "       [1.64049603e-02, 4.64993328e-01, 5.18601716e-01],\n",
              "       [9.69090939e-01, 3.07659488e-02, 1.43121171e-04],\n",
              "       [9.78829503e-01, 2.10755579e-02, 9.49423556e-05],\n",
              "       [1.84862167e-02, 4.42029506e-01, 5.39484322e-01],\n",
              "       [1.42407529e-02, 4.13140804e-01, 5.72618425e-01],\n",
              "       [1.14362296e-02, 4.93128836e-01, 4.95434910e-01],\n",
              "       [9.77661192e-01, 2.22454593e-02, 9.34012423e-05],\n",
              "       [2.31755059e-03, 2.33045653e-01, 7.64636815e-01],\n",
              "       [4.15691733e-02, 6.38535202e-01, 3.19895685e-01],\n",
              "       [9.91405606e-01, 8.58012121e-03, 1.42633307e-05],\n",
              "       [9.77653146e-01, 2.22591180e-02, 8.77662242e-05],\n",
              "       [1.98983215e-02, 5.12784541e-01, 4.67317253e-01],\n",
              "       [3.66119505e-03, 2.50261575e-01, 7.46077240e-01],\n",
              "       [2.30343584e-02, 5.66498220e-01, 4.10467356e-01],\n",
              "       [1.16006029e-03, 1.93451360e-01, 8.05388510e-01],\n",
              "       [5.23395203e-02, 6.55265272e-01, 2.92395115e-01],\n",
              "       [1.00007898e-03, 1.24936707e-01, 8.74063194e-01],\n",
              "       [1.20322814e-03, 1.33164540e-01, 8.65632236e-01],\n",
              "       [9.81731057e-01, 1.82057787e-02, 6.31831936e-05],\n",
              "       [2.60884017e-02, 5.25519431e-01, 4.48392153e-01],\n",
              "       [9.78348315e-01, 2.15598196e-02, 9.18592850e-05]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMR5rcTbkQYP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c7392d34-8dff-423f-bc66-3be3bbfa7dcc"
      },
      "source": [
        "y_predict[1]"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.18219236, 0.7030589 , 0.11474869], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMCEkgzwkUGu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca24c46e-4614-482e-9834-2bd1f82454d1"
      },
      "source": [
        "np.argmax(y_predict[1])"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1IqftFppk1X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "253a2267-73b5-4775-856c-6c9af0838c71"
      },
      "source": [
        "y_test[1]"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSUgMq3m0bG7",
        "colab_type": "text"
      },
      "source": [
        "### Compare the prediction with actual label\n",
        "- Print the same row as done in the previous step but of actual labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5WbwVPyz-qQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "10c9d35c-88f6-4c48-ad55-853825e01d45"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "y_pred = []\n",
        "for val in y_predict:\n",
        "    y_pred.append(np.argmax(val))\n",
        "#print(y_pred)    \n",
        "#convert 0 1 to 1 and 1 0 as 0\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "print(cm)"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[13  0  0]\n",
            " [ 0 12  4]\n",
            " [ 0  0  9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrTKwbgE7NFT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeoiMbH7djEv",
        "colab_type": "text"
      },
      "source": [
        "# Stock prices dataset\n",
        "The data is of tock exchange's stock listings for each trading day of 2010 to 2016.\n",
        "\n",
        "## Description\n",
        "A brief description of columns.\n",
        "- open: The opening market price of the equity symbol on the date\n",
        "- high: The highest market price of the equity symbol on the date\n",
        "- low: The lowest recorded market price of the equity symbol on the date\n",
        "- close: The closing recorded price of the equity symbol on the date\n",
        "- symbol: Symbol of the listed company\n",
        "- volume: Total traded volume of the equity symbol on the date\n",
        "- date: Date of record"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSoYY2nII_UW",
        "colab_type": "text"
      },
      "source": [
        "In this assignment, we will work on the stock prices dataset named \"prices.csv\". Task is to create a Neural Network to classify closing price for a stock based on some parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v9Ws2l6jdLa_"
      },
      "source": [
        "Firstly, let's select TensorFlow version 2.x in colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "407SiobOdLbL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e97c1e26-3f66-4361-f1e2-aeaeb5f2e69d"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow\n",
        "tensorflow.__version__"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PGqq9f8VdLba",
        "colab": {}
      },
      "source": [
        "# Initialize the random number generator\n",
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "# Ignore the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_88voqAH-O6J",
        "colab_type": "text"
      },
      "source": [
        "## Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRHCeJqP-evf",
        "colab_type": "text"
      },
      "source": [
        "### Load the data\n",
        "- load the csv file and read it using pandas\n",
        "- file name is prices.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKVH5v7r-RmC",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "3037fc9d-8b71-4246-d18f-3890df252c81"
      },
      "source": [
        "# run this cell to upload file using GUI if you are using google colab\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-411c575f-3fb9-4631-b0a4-6c9730c03d77\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-411c575f-3fb9-4631-b0a4-6c9730c03d77\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving prices.csv to prices (3).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr4YcffYd1FQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "732beeda-0c5a-42f1-bb3c-c9a165bf012a"
      },
      "source": [
        "# run this cell to to mount the google drive if you are using google colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gDC6cSW_FSK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "ffb8eee3-c99f-4cb5-b86d-ea2442a42f28"
      },
      "source": [
        "DF=pd.read_csv('prices (3).csv')\n",
        "DF.head()"
      ],
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>symbol</th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-01-05 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-01-06 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-01-07 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-01-08 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-01-11 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  date symbol        open  ...         low        high     volume\n",
              "0  2016-01-05 00:00:00   WLTW  123.430000  ...  122.309998  126.250000  2163600.0\n",
              "1  2016-01-06 00:00:00   WLTW  125.239998  ...  119.940002  125.540001  2386400.0\n",
              "2  2016-01-07 00:00:00   WLTW  116.379997  ...  114.930000  119.739998  2489500.0\n",
              "3  2016-01-08 00:00:00   WLTW  115.480003  ...  113.500000  117.440002  2006300.0\n",
              "4  2016-01-11 00:00:00   WLTW  117.010002  ...  114.089996  117.330002  1408600.0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN2swUgQxn65",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b2e92cda-2390-4a1f-84f9-3f1f186089f0"
      },
      "source": [
        "DF.shape"
      ],
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(851264, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 269
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlLKVPVH_BCT",
        "colab_type": "text"
      },
      "source": [
        "## Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZxoGynuBeO4t"
      },
      "source": [
        "### Drop null\n",
        "- Drop null values if any"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yuwJJIeeUaD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "0110c602-71ca-4247-94b2-5d486666f564"
      },
      "source": [
        "DF.isnull().sum()"
      ],
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "date      0\n",
              "symbol    0\n",
              "open      0\n",
              "close     0\n",
              "low       0\n",
              "high      0\n",
              "volume    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 270
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbIwwLuHxZEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J4BlzVA_gZd",
        "colab_type": "text"
      },
      "source": [
        "### Drop columnns\n",
        "- Now, we don't need \"date\", \"volume\" and \"symbol\" column\n",
        "- drop \"date\", \"volume\" and \"symbol\" column from the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKEK8aEE_Csx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DF.drop(['date','volume','symbol'],inplace=True,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTPhO6v-AiZt",
        "colab_type": "text"
      },
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsZXmF3NAkna",
        "colab_type": "text"
      },
      "source": [
        "### Print the dataframe\n",
        "- print the modified dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKs04iIHAjxN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "1548e594-32fe-4d60-d9ca-2fef9a76c72f"
      },
      "source": [
        "DF.head()"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         open       close         low        high\n",
              "0  123.430000  125.839996  122.309998  126.250000\n",
              "1  125.239998  119.980003  119.940002  125.540001\n",
              "2  116.379997  114.949997  114.930000  119.739998\n",
              "3  115.480003  116.620003  113.500000  117.440002\n",
              "4  117.010002  114.970001  114.089996  117.330002"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 272
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C8u_jlbABTip"
      },
      "source": [
        "### Get features and label from the dataset in separate variable\n",
        "- Let's separate labels and features now. We are going to predict the value for \"close\" column so that will be our label. Our features will be \"open\", \"low\", \"high\"\n",
        "- Take \"open\" \"low\", \"high\" columns as features\n",
        "- Take \"close\" column as label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQjCMzUXBJbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=DF.drop('close',axis=1)\n",
        "y=np.ravel(DF.close)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vGtnapgBIJm",
        "colab_type": "text"
      },
      "source": [
        "## Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pZAKdJ5gcrm",
        "colab_type": "text"
      },
      "source": [
        "### Create train and test sets\n",
        "- Split the data into training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KalRqA6Rgqsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data up in train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Aw1lEajy0jg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "382d8f2c-22b6-4b73-86bc-7831c3a35376"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(595884, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 275
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owERoRuhy7LK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "154c7663-ccb2-430b-c9dd-c539cab00f14"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(255380, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 276
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTAKzlxZBz0z",
        "colab_type": "text"
      },
      "source": [
        "## Question 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7BU2qxEg0Ki",
        "colab_type": "text"
      },
      "source": [
        "### Scaling\n",
        "- Scale the data (features only)\n",
        "- Use StandarScaler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcO8SlhPhBkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3TWpN0nVTpUx"
      },
      "source": [
        "## Question 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sj0LYNkhR-L",
        "colab_type": "text"
      },
      "source": [
        "### Convert data to NumPy array\n",
        "- Convert features and labels to numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYffLFiG0sY9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b05a9408-8719-443c-c498-367908935250"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(595884, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 278
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbESkpe7hiVk",
        "colab_type": "text"
      },
      "source": [
        "### Reshape features\n",
        "- Reshape the features to make it suitable for input in the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_ZG_CWthpTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xtrain=X_train.reshape(X_train.shape[0],X_train.shape[1],1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u2ZMcE07dfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xtest=X_test.reshape(X_test.shape[0],X_test.shape[1],1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbDWPw1b1U3D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "365200ed-0412-455a-981c-aabbbd3e980b"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(595884, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 281
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wmXUGc2oTspa"
      },
      "source": [
        "## Question 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl2M9whFh6mh",
        "colab_type": "text"
      },
      "source": [
        "### Define Model\n",
        "- Initialize a Sequential model\n",
        "- Add a Flatten layer\n",
        "- Add a Dense layer with one neuron as output\n",
        "  - add 'linear' as activation function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkiBpORmiegL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.layers.advanced_activations import ReLU\n",
        "\n",
        "# define the model architecture\n",
        "\n",
        "# Initialize the constructor\n",
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NduKPjIb2pvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add an input layer \n",
        "model.add(Flatten())\n",
        "\n",
        "# Add an output layer \n",
        "model.add(Dense(1, activation='linear'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8a0wr94aTyjg"
      },
      "source": [
        "## Question 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNZPb5lKioX0",
        "colab_type": "text"
      },
      "source": [
        "### Compile the model\n",
        "- Compile the model\n",
        "- Use \"sgd\" optimizer\n",
        "- for calculating loss, use mean squared error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEQUP3VaiuT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='mse',optimizer='sgd',metrics=['mae', 'mse'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZbBpnOtfT0wd"
      },
      "source": [
        "## Question 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9o45OHdjDhA",
        "colab_type": "text"
      },
      "source": [
        "### Fit the model\n",
        "- epochs: 50\n",
        "- batch size: 128\n",
        "- specify validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y6tA30XjOH2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e547ed7-6094-4431-eee2-7804d6460ef9"
      },
      "source": [
        "epochs = 50\n",
        "batch_size = 128\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=.3, verbose=True)\n",
        "mse = tf.keras.losses.MeanSquaredError()"
      ],
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 417118 samples, validate on 178766 samples\n",
            "Epoch 1/50\n",
            "417118/417118 [==============================] - 6s 15us/sample - loss: 57.3682 - mae: 1.5303 - mse: 57.3683 - val_loss: 0.9629 - val_mae: 0.4826 - val_mse: 0.9629\n",
            "Epoch 2/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.9664 - mae: 0.4816 - mse: 0.9664 - val_loss: 0.9605 - val_mae: 0.4835 - val_mse: 0.9605\n",
            "Epoch 3/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.9616 - mae: 0.4802 - mse: 0.9616 - val_loss: 0.9656 - val_mae: 0.4867 - val_mse: 0.9656\n",
            "Epoch 4/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.9547 - mae: 0.4796 - mse: 0.9547 - val_loss: 1.0204 - val_mae: 0.4973 - val_mse: 1.0204\n",
            "Epoch 5/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.9514 - mae: 0.4786 - mse: 0.9514 - val_loss: 0.9424 - val_mae: 0.4782 - val_mse: 0.9424\n",
            "Epoch 6/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.9461 - mae: 0.4775 - mse: 0.9461 - val_loss: 0.9433 - val_mae: 0.4801 - val_mse: 0.9433\n",
            "Epoch 7/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.9417 - mae: 0.4764 - mse: 0.9417 - val_loss: 0.9394 - val_mae: 0.4788 - val_mse: 0.9394\n",
            "Epoch 8/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.9372 - mae: 0.4753 - mse: 0.9372 - val_loss: 0.9303 - val_mae: 0.4756 - val_mse: 0.9303\n",
            "Epoch 9/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.9335 - mae: 0.4741 - mse: 0.9335 - val_loss: 0.9262 - val_mae: 0.4754 - val_mse: 0.9262\n",
            "Epoch 10/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.9279 - mae: 0.4729 - mse: 0.9279 - val_loss: 0.9477 - val_mae: 0.4822 - val_mse: 0.9477\n",
            "Epoch 11/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.9227 - mae: 0.4725 - mse: 0.9227 - val_loss: 0.9233 - val_mae: 0.4736 - val_mse: 0.9233\n",
            "Epoch 12/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.9192 - mae: 0.4709 - mse: 0.9192 - val_loss: 0.9457 - val_mae: 0.4783 - val_mse: 0.9457\n",
            "Epoch 13/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.9125 - mae: 0.4704 - mse: 0.9125 - val_loss: 0.9087 - val_mae: 0.4713 - val_mse: 0.9087\n",
            "Epoch 14/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.9117 - mae: 0.4690 - mse: 0.9117 - val_loss: 0.9026 - val_mae: 0.4690 - val_mse: 0.9026\n",
            "Epoch 15/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.9052 - mae: 0.4681 - mse: 0.9052 - val_loss: 0.9106 - val_mae: 0.4701 - val_mse: 0.9106\n",
            "Epoch 16/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8998 - mae: 0.4675 - mse: 0.8998 - val_loss: 0.9225 - val_mae: 0.4734 - val_mse: 0.9225\n",
            "Epoch 17/50\n",
            "417118/417118 [==============================] - 6s 16us/sample - loss: 0.8970 - mae: 0.4661 - mse: 0.8970 - val_loss: 0.8941 - val_mae: 0.4663 - val_mse: 0.8941\n",
            "Epoch 18/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8937 - mae: 0.4650 - mse: 0.8937 - val_loss: 0.8984 - val_mae: 0.4702 - val_mse: 0.8984\n",
            "Epoch 19/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8896 - mae: 0.4642 - mse: 0.8896 - val_loss: 0.8844 - val_mae: 0.4641 - val_mse: 0.8844\n",
            "Epoch 20/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8863 - mae: 0.4631 - mse: 0.8863 - val_loss: 0.8894 - val_mae: 0.4652 - val_mse: 0.8894\n",
            "Epoch 21/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8802 - mae: 0.4624 - mse: 0.8802 - val_loss: 0.9457 - val_mae: 0.4802 - val_mse: 0.9457\n",
            "Epoch 22/50\n",
            "417118/417118 [==============================] - 6s 15us/sample - loss: 0.8752 - mae: 0.4615 - mse: 0.8752 - val_loss: 0.8699 - val_mae: 0.4612 - val_mse: 0.8699\n",
            "Epoch 23/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8726 - mae: 0.4604 - mse: 0.8726 - val_loss: 0.8665 - val_mae: 0.4602 - val_mse: 0.8665\n",
            "Epoch 24/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8700 - mae: 0.4594 - mse: 0.8700 - val_loss: 0.8649 - val_mae: 0.4595 - val_mse: 0.8649\n",
            "Epoch 25/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8638 - mae: 0.4584 - mse: 0.8638 - val_loss: 0.8581 - val_mae: 0.4584 - val_mse: 0.8581\n",
            "Epoch 26/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8605 - mae: 0.4577 - mse: 0.8605 - val_loss: 0.8690 - val_mae: 0.4607 - val_mse: 0.8690\n",
            "Epoch 27/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8576 - mae: 0.4565 - mse: 0.8576 - val_loss: 0.8535 - val_mae: 0.4581 - val_mse: 0.8535\n",
            "Epoch 28/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8520 - mae: 0.4558 - mse: 0.8520 - val_loss: 0.8467 - val_mae: 0.4556 - val_mse: 0.8467\n",
            "Epoch 29/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8500 - mae: 0.4546 - mse: 0.8500 - val_loss: 0.8668 - val_mae: 0.4652 - val_mse: 0.8668\n",
            "Epoch 30/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8460 - mae: 0.4537 - mse: 0.8461 - val_loss: 0.8413 - val_mae: 0.4548 - val_mse: 0.8413\n",
            "Epoch 31/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8427 - mae: 0.4527 - mse: 0.8427 - val_loss: 0.8365 - val_mae: 0.4528 - val_mse: 0.8365\n",
            "Epoch 32/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8398 - mae: 0.4515 - mse: 0.8398 - val_loss: 0.8458 - val_mae: 0.4560 - val_mse: 0.8458\n",
            "Epoch 33/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8343 - mae: 0.4508 - mse: 0.8343 - val_loss: 0.8334 - val_mae: 0.4540 - val_mse: 0.8334\n",
            "Epoch 34/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8314 - mae: 0.4501 - mse: 0.8314 - val_loss: 0.8269 - val_mae: 0.4502 - val_mse: 0.8269\n",
            "Epoch 35/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8261 - mae: 0.4495 - mse: 0.8261 - val_loss: 0.8213 - val_mae: 0.4493 - val_mse: 0.8213\n",
            "Epoch 36/50\n",
            "417118/417118 [==============================] - 7s 16us/sample - loss: 0.8234 - mae: 0.4486 - mse: 0.8234 - val_loss: 0.8420 - val_mae: 0.4543 - val_mse: 0.8420\n",
            "Epoch 37/50\n",
            "417118/417118 [==============================] - 6s 15us/sample - loss: 0.8209 - mae: 0.4476 - mse: 0.8209 - val_loss: 0.8419 - val_mae: 0.4544 - val_mse: 0.8419\n",
            "Epoch 38/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8148 - mae: 0.4469 - mse: 0.8148 - val_loss: 0.8132 - val_mae: 0.4468 - val_mse: 0.8132\n",
            "Epoch 39/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8143 - mae: 0.4457 - mse: 0.8143 - val_loss: 0.8144 - val_mae: 0.4473 - val_mse: 0.8144\n",
            "Epoch 40/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8087 - mae: 0.4450 - mse: 0.8087 - val_loss: 0.8091 - val_mae: 0.4477 - val_mse: 0.8091\n",
            "Epoch 41/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8059 - mae: 0.4440 - mse: 0.8059 - val_loss: 0.8014 - val_mae: 0.4443 - val_mse: 0.8014\n",
            "Epoch 42/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8038 - mae: 0.4430 - mse: 0.8038 - val_loss: 0.8059 - val_mae: 0.4456 - val_mse: 0.8059\n",
            "Epoch 43/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.8010 - mae: 0.4421 - mse: 0.8010 - val_loss: 0.7975 - val_mae: 0.4446 - val_mse: 0.7975\n",
            "Epoch 44/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.7966 - mae: 0.4416 - mse: 0.7966 - val_loss: 0.7928 - val_mae: 0.4417 - val_mse: 0.7928\n",
            "Epoch 45/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.7933 - mae: 0.4406 - mse: 0.7933 - val_loss: 0.7878 - val_mae: 0.4410 - val_mse: 0.7878\n",
            "Epoch 46/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.7894 - mae: 0.4400 - mse: 0.7894 - val_loss: 0.7959 - val_mae: 0.4434 - val_mse: 0.7959\n",
            "Epoch 47/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.7865 - mae: 0.4390 - mse: 0.7865 - val_loss: 0.7983 - val_mae: 0.4427 - val_mse: 0.7983\n",
            "Epoch 48/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.7838 - mae: 0.4383 - mse: 0.7838 - val_loss: 0.7824 - val_mae: 0.4394 - val_mse: 0.7824\n",
            "Epoch 49/50\n",
            "417118/417118 [==============================] - 6s 14us/sample - loss: 0.7799 - mae: 0.4373 - mse: 0.7799 - val_loss: 0.7772 - val_mae: 0.4388 - val_mse: 0.7772\n",
            "Epoch 50/50\n",
            "417118/417118 [==============================] - 6s 15us/sample - loss: 0.7773 - mae: 0.4365 - mse: 0.7773 - val_loss: 0.7733 - val_mae: 0.4376 - val_mse: 0.7733\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AW4SEP8kT2ls"
      },
      "source": [
        "## Question 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJDoix_7JU61",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate the model\n",
        "- Evaluate the model on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdH8pYBIjHGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_predict = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cwb0TVtw8zVV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "16b3cb6c-8998-4ef5-e632-e9991a6834d0"
      },
      "source": [
        "loss, mae, mse = model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "print(\"Testing set Mean Abs Error:\", mae)"
      ],
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "255380/255380 - 7s - loss: 0.8114 - mae: 0.4381 - mse: 0.8114\n",
            "Testing set Mean Abs Error: 0.43812275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUpDD74Xjh01",
        "colab_type": "text"
      },
      "source": [
        "### Manual predictions\n",
        "- Test the predictions on manual inputs\n",
        "- We have scaled out training data, so we need to transform our custom inputs using the object of the scaler\n",
        "- Example of manual input: [123.430000,\t122.30999, 116.250000]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvuH-c31lLiJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69ad88ee-8a27-46fe-96aa-135d80ae3662"
      },
      "source": [
        "y_predict[1]"
      ],
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([292.50278], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 294
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSiMhJPpk3GO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e63d962b-4d91-426c-8d02-b28a17fd1a23"
      },
      "source": [
        "y_test[1]"
      ],
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "292.440002"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 295
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWfNjEJs_0iT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "man_inp=np.array([123.430000, 122.309998, 126.25000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLj1vOl0_3ww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "man_inp=man_inp.reshape(1,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYhiewi9AW5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "man_inp=scaler.transform(man_inp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT61rfRMAmVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "man_pred=model.predict(man_inp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3EPFHddAso9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4385e644-76a1-4841-be74-816917f9fbcb"
      },
      "source": [
        "man_pred"
      ],
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[124.133514]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 310
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2L3T4e5AwVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
